{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1wyepc9bofaiGrzdSz/cg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malfaroe/NeuralNetworksMae/blob/main/NeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GOKGPtJx2tOM",
        "outputId": "58e8dd7b-83ae-442b-c774-c2762c86fa26"
      },
      "source": [
        "\"\"\"Implementation of building blocks for a neural network architecture\n",
        "url: https://www.youtube.com/watch?v=pauPCy_s0Ok&ab_channel=TheIndependentCode\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Implementation of building blocks for a neural network architecture\\nurl: https://www.youtube.com/watch?v=pauPCy_s0Ok&ab_channel=TheIndependentCode'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FfJPS3728Nk"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz8UDVyP2_iT"
      },
      "source": [
        "from types import prepare_class\n",
        "\"\"\"Base layer:\n",
        "\n",
        "Architecture of a layer\n",
        "\n",
        "class has two directions:\n",
        "\n",
        "Forward:\n",
        "--------\n",
        "Input: x --- input values\n",
        "Output: y ---output values\n",
        "\n",
        "Backwards:\n",
        "---------\n",
        "Input: dE/dy\n",
        "Outputs:\n",
        "- Updated weights\n",
        "- dE/dx (which is the input for the previous layer backwrads direction\"\"\"\n",
        "\n",
        "\n",
        "class Layer():\n",
        "    \"\"\"Base layer class structure\"\"\"\n",
        "    def __init__(self):\n",
        "        self.input = None \n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Input: input (x)\n",
        "        Output: y\"\"\"\n",
        "        #TO DO: return output value(y)\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        \"\"\"\n",
        "        Input: output_gradient dE/dy \n",
        "        Output:  updated_weights and  dE/dx\n",
        "        #TO DO: update weights and return dE/dx\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"The value of the output in forward movement:\n",
        "Y = W.dot(X) + B\n",
        "were:\n",
        "Y (output_value) = Output_size x 1\n",
        "X = Input_size x 1\n",
        "B = Output_size x 1\n",
        "weights_size = Output_size x Input_size\"\"\"\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    #  \"\"\"Dense layer (or fully connected layer) \n",
        "    # implementation that inherits from class Layer\"\"\"\n",
        "    def __init__(self, output_size, input_size):\n",
        "        self.weights = np.random.rand(output_size, input_size)\n",
        "        self.bias = np.random.rand(output_size, 1)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Computes the output value of the layer\"\"\"\n",
        "    #TO DO: return output (y)\n",
        "        self.input = input #asigna un valor al atributo input que era None\n",
        "        return np.dot(self.weights, input) + self.bias\n",
        "            \n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        \"\"\"\n",
        "        Input: output_gradient dE/dy \n",
        "        Output:  updated weights and  dE/dx\n",
        "        dE/dW = dE/dy.dot(Xt)\n",
        "        dE/dB = dE/dy\n",
        "        dE/dx = Wt.dot(dE/dy)\n",
        "\n",
        "        w = w_anterior - learning_rate * dE/dw\n",
        "        b = b_anterior - learning_rate * dE/dB\n",
        "        dE/dx = weights_transposed.dot(dE/dy)\n",
        "\n",
        "        TO DO: update weights and return dE/dx\"\"\"\n",
        "\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T) #dE/dW\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        bias_gradient = output_gradient\n",
        "        self.bias -= learning_rate * bias_gradient\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient) #dE/dx\n",
        "        return input_gradient\n",
        "\n",
        "\n",
        "class Activation(Layer):\n",
        "    \"\"\"Activation layer:\n",
        "    Como es una capa, tiene dos direcciones:\n",
        "    forward: aplica una funcion de activacion al input X\n",
        "    backward: Devuelve el dE/dx\"\"\"\n",
        "\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(self.input)\n",
        "\n",
        "    def backward(self,output_gradient, learning_rate):\n",
        "        \"\"\"dE/dx = dE/dactivation * dActivation/dx (=activation_prime)\"\"\"\n",
        "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
        "        \n",
        "\n",
        "##Activation function: tanh\n",
        "\"\"\"tanh(x) = (e**x - e**-x) / (e**x + e**-x)\n",
        "dervada tanh = 1 - tanh**2\"\"\"\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        tanh = lambda x: np.tanh(x)\n",
        "        tanh_prime = lambda x: 1 - np.tanh(x) ** 2\n",
        "        super().__init__(tanh, tanh_prime) \n",
        "        \"\"\"Here, youâ€™ve used super() to call the __init__() of the\n",
        "        Activation class, allowing you to use it in the Tanh class \n",
        "        without repeating code\n",
        "        url: https://realpython.com/python-super/\"\"\"\n",
        "\n",
        "\n",
        "##Error function (Cost Function) and output_gradient (dE/dY)\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "def mse_prime(y_true, y_pred): #dE/dY\n",
        "    return 2 *(y_pred - y_true)/len(y_true)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8ZDwAXQ42bf",
        "outputId": "1f3bdeec-8e9c-4c20-f90f-01f3c6a6f22b"
      },
      "source": [
        "ly = Layer()\n",
        "cd = Dense(output_size= 5, input_size= 7)\n",
        "output_value_vector = cd.forward(input = np.random.rand(7,1))\n",
        "np.shape(output_value_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKQ-spx8t3--",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0ee7ca2-f1ec-4398-ef77-f2454aea055b"
      },
      "source": [
        "\"\"\"LEARNING POINTS---\n",
        "- classes inheritance\n",
        "- declaring None to variables inside a class\n",
        " \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LEARNING POINTS---\\n- classes inheritance\\n- declaring None to variables inside a class\\n '"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0Yvpgy8T2yW",
        "outputId": "8106921e-3035-4405-a5d6-b374ea9c2508"
      },
      "source": [
        "x = np.random.rand(3,1)\n",
        "y_true = np.random.rand(5,1)\n",
        "\n",
        "ly = Layer()\n",
        "cd = Dense(output_size= y_true.shape[0], input_size= x.shape[0])\n",
        "output_value_vector = cd.forward(input = x)\n",
        "np.shape(output_value_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30Qz01NeNXW3",
        "outputId": "d171c93d-cc29-48cf-c920-d4e7834b5a2c"
      },
      "source": [
        "#Ensamble the network architecture\n",
        "network = [Dense(output_size= y_true.shape[0], input_size= x.shape[0]), Tanh()]\n",
        "lr = 0.1\n",
        "input = x\n",
        "for layer in network:\n",
        "    input = layer.forward(input)\n",
        "   \n",
        "\n",
        "input\n",
        "\n",
        "#luego con este input saco el error, error_prime (dE/dy) y backpropago\n",
        "y_pred = input\n",
        "error = mse(y_true, y_pred)\n",
        "print(\"error iteration:\", error)\n",
        "output_gradient = mse_prime(y_true, y_pred)\n",
        "\n",
        "#backpropagation\n",
        "for layer in reversed(network):\n",
        "    layer.backward(output_gradient, lr)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error iteration: 0.25219786033411395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN6BTVB1T9N_",
        "outputId": "c37857b1-2d9c-4316-9bc2-6c7372ecae31"
      },
      "source": [
        "    #Ensamble the network architecture\n",
        "network = [Dense(output_size= y_true.shape[0], input_size= x.shape[0]), Tanh()]\n",
        "lr = 0.1\n",
        "epochs = 100\n",
        "for e in range(epochs):\n",
        "\n",
        "        input = x\n",
        "        for layer in network:\n",
        "            input = layer.forward(input)\n",
        "        #luego con este input saco el error, error_prime (dE/dy) y backpropago\n",
        "        y_pred = input\n",
        "        error = mse(y_true, y_pred)\n",
        "        print('%d/%d, error=%f' % (e + 1, epochs, error))\n",
        "        output_gradient = mse_prime(y_true, y_pred)\n",
        "\n",
        "        #backpropagation\n",
        "        for layer in reversed(network):\n",
        "            layer.backward(output_gradient, lr)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/100, error=0.366609\n",
            "2/100, error=0.354551\n",
            "3/100, error=0.341944\n",
            "4/100, error=0.328828\n",
            "5/100, error=0.315251\n",
            "6/100, error=0.301271\n",
            "7/100, error=0.286953\n",
            "8/100, error=0.272373\n",
            "9/100, error=0.257614\n",
            "10/100, error=0.242764\n",
            "11/100, error=0.227914\n",
            "12/100, error=0.213160\n",
            "13/100, error=0.198596\n",
            "14/100, error=0.184313\n",
            "15/100, error=0.170398\n",
            "16/100, error=0.156932\n",
            "17/100, error=0.143986\n",
            "18/100, error=0.131621\n",
            "19/100, error=0.119886\n",
            "20/100, error=0.108819\n",
            "21/100, error=0.098446\n",
            "22/100, error=0.088779\n",
            "23/100, error=0.079821\n",
            "24/100, error=0.071563\n",
            "25/100, error=0.063991\n",
            "26/100, error=0.057078\n",
            "27/100, error=0.050795\n",
            "28/100, error=0.045109\n",
            "29/100, error=0.039982\n",
            "30/100, error=0.035374\n",
            "31/100, error=0.031247\n",
            "32/100, error=0.027561\n",
            "33/100, error=0.024277\n",
            "34/100, error=0.021359\n",
            "35/100, error=0.018771\n",
            "36/100, error=0.016481\n",
            "37/100, error=0.014457\n",
            "38/100, error=0.012672\n",
            "39/100, error=0.011100\n",
            "40/100, error=0.009716\n",
            "41/100, error=0.008501\n",
            "42/100, error=0.007433\n",
            "43/100, error=0.006497\n",
            "44/100, error=0.005677\n",
            "45/100, error=0.004958\n",
            "46/100, error=0.004330\n",
            "47/100, error=0.003780\n",
            "48/100, error=0.003299\n",
            "49/100, error=0.002878\n",
            "50/100, error=0.002511\n",
            "51/100, error=0.002191\n",
            "52/100, error=0.001911\n",
            "53/100, error=0.001667\n",
            "54/100, error=0.001454\n",
            "55/100, error=0.001268\n",
            "56/100, error=0.001106\n",
            "57/100, error=0.000965\n",
            "58/100, error=0.000841\n",
            "59/100, error=0.000734\n",
            "60/100, error=0.000640\n",
            "61/100, error=0.000559\n",
            "62/100, error=0.000487\n",
            "63/100, error=0.000425\n",
            "64/100, error=0.000371\n",
            "65/100, error=0.000324\n",
            "66/100, error=0.000283\n",
            "67/100, error=0.000247\n",
            "68/100, error=0.000216\n",
            "69/100, error=0.000189\n",
            "70/100, error=0.000165\n",
            "71/100, error=0.000144\n",
            "72/100, error=0.000126\n",
            "73/100, error=0.000111\n",
            "74/100, error=0.000097\n",
            "75/100, error=0.000085\n",
            "76/100, error=0.000074\n",
            "77/100, error=0.000065\n",
            "78/100, error=0.000057\n",
            "79/100, error=0.000050\n",
            "80/100, error=0.000044\n",
            "81/100, error=0.000039\n",
            "82/100, error=0.000034\n",
            "83/100, error=0.000030\n",
            "84/100, error=0.000027\n",
            "85/100, error=0.000023\n",
            "86/100, error=0.000021\n",
            "87/100, error=0.000018\n",
            "88/100, error=0.000016\n",
            "89/100, error=0.000014\n",
            "90/100, error=0.000013\n",
            "91/100, error=0.000011\n",
            "92/100, error=0.000010\n",
            "93/100, error=0.000009\n",
            "94/100, error=0.000008\n",
            "95/100, error=0.000007\n",
            "96/100, error=0.000006\n",
            "97/100, error=0.000006\n",
            "98/100, error=0.000005\n",
            "99/100, error=0.000005\n",
            "100/100, error=0.000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26K48UcgUwue"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}