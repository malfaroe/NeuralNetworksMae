{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BUILDING AN STRUCTURE FOR RELIABLE, CLEAR AND REPRODUCIBLE EDA\n",
    "\n",
    "#Objectives:\n",
    "#1. Generate a pipeline of techniques for analyzing and getting the best dataset possible for modeling stage\n",
    "#2. Learn and incorporate all techniques I can find "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime, timedelta\n",
    "%matplotlib inline\n",
    "\n",
    "# import all libraries and dependencies for data visualization\n",
    "pd.options.display.float_format='{:.4f}'.format\n",
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "pd.set_option('display.max_columns', 350)\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "sns.set(style='darkgrid')\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "\n",
    "# import all libraries and dependencies for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression, OrthogonalMatchingPursuit, Lasso, LassoLarsIC, ElasticNet, ElasticNetCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest, RFECV, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, kurtosis, skew\n",
    "\n",
    "# Import specific libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats import diagnostic as diag\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest, RFECV, SelectFromModel\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, OrthogonalMatchingPursuit, Lasso, LassoLarsIC, ElasticNet, ElasticNetCV\n",
    "from sklearn.linear_model import SGDRegressor, PassiveAggressiveRegressor, HuberRegressor, BayesianRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "import mlxtend\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import lightgbm as lgb\n",
    "# Models\n",
    "import mlxtend\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm, probplot, boxcox\n",
    "from scipy.special import boxcox1p\n",
    "from patsy import dmatrices\n",
    "\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "\n",
    "#For baseline estimations\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#For importing from my own libraries in src :)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# in jupyter (lab / notebook), based on notebook path\n",
    "module_path = str(Path.cwd().parents[0] / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from house_utils import data_summary, features_profile, plot_feats\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from itertools import combinations \n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "###TO PREVENT SCROLLING : SHIFT + O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##UTILS FOR DATA PROCESSING\n",
    "\n",
    "def rescaler(df, target):\n",
    "#Rescaling utility\n",
    "    y = df.pop(target)\n",
    "    X = df\n",
    "    test_ID = y.index\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    XRescaled = scaler.transform(X)\n",
    "    df_rescaled = pd.DataFrame(XRescaled, columns = X.columns, index = test_ID)\n",
    "    df = pd.concat((y, df_rescaled), axis = 1)\n",
    "    df.set_index(test_ID)\n",
    "    print(\"Data has been rescaled...\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0000</td>\n",
       "      <td>128.0000</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Rural</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0000</td>\n",
       "      <td>120.0000</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>141.0000</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History Property_Area  Loan_Status\n",
       "0  LP001002  Male   No      0          Graduate      No            5849            0.0000             nan         360.0000          1.0000           Urban         1          \n",
       "1  LP001003  Male   Yes     1          Graduate      No            4583            1508.0000          128.0000    360.0000          1.0000           Rural         0          \n",
       "2  LP001005  Male   Yes     0          Graduate      Yes           3000            0.0000             66.0000     360.0000          1.0000           Urban         1          \n",
       "3  LP001006  Male   Yes     0          Not Graduate  No            2583            2358.0000          120.0000    360.0000          1.0000           Urban         1          \n",
       "4  LP001008  Male   No      0          Graduate      No            6000            0.0000             141.0000    360.0000          1.0000           Urban         1          "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ THE WHOLE DATASET:\n",
    "# We load and read the whole dataset (train + test) because we need to clean and preprocess all of it!!!!\n",
    "#All the procedures must be applied to the entire dataset before split it!\n",
    "\n",
    "#Step 1: Load train and test sets\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#Target\n",
    "target = \"Loan_Status\"\n",
    "\n",
    "#Change target type\n",
    "dict = {\"Y\":1, \"N\":0}\n",
    "train[target] = train[target].map(dict)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the 'Id' column\n",
    "train_ID = train['Loan_ID']\n",
    "test_ID = test['Loan_ID']\n",
    "\n",
    "test_ID.to_csv(\"test_ID.csv\", index = False)\n",
    "\n",
    "#Drop ID from both datasets\n",
    "# train.drop(\"Id\", axis = 1, inplace = True)\n",
    "# test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "# Shape of the datasets\n",
    "train.shape, test.shape, train.shape[0], test.shape[0]\n",
    "\n",
    "#Separar y convertir el target en DataFrame\n",
    "y_train = train[target].to_frame()\n",
    "\n",
    "#Step 2: Concatenate both datasets into one\n",
    "\n",
    "df = pd.concat((train,test), sort = False).reset_index(drop = True)\n",
    "\n",
    "#drop target and id columns\n",
    "df.drop(columns = [target, \"Loan_ID\"], axis = 1, inplace = True)\n",
    "train.drop(columns = [\"Loan_ID\"], axis = 1, inplace = True)\n",
    "test.drop(columns = [\"Loan_ID\"], axis = 1, inplace = True)\n",
    "\n",
    "target = \"Loan_Status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Gender             601 non-null    object \n",
      " 1   Married            611 non-null    object \n",
      " 2   Dependents         599 non-null    object \n",
      " 3   Education          614 non-null    object \n",
      " 4   Self_Employed      582 non-null    object \n",
      " 5   ApplicantIncome    614 non-null    int64  \n",
      " 6   CoapplicantIncome  614 non-null    float64\n",
      " 7   LoanAmount         592 non-null    float64\n",
      " 8   Loan_Amount_Term   600 non-null    float64\n",
      " 9   Credit_History     564 non-null    float64\n",
      " 10  Property_Area      614 non-null    object \n",
      " 11  Loan_Status        614 non-null    int64  \n",
      "dtypes: float64(4), int64(2), object(6)\n",
      "memory usage: 57.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#Variable identificacion\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labelencoding target\n",
    "le = LabelEncoder()\n",
    "train[\"Loan_Status\"] = pd.Series(le.fit_transform(train[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0000</td>\n",
       "      <td>128.0000</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Rural</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender Married Dependents Education Self_Employed  ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History Property_Area  Loan_Status\n",
       "0  Male   No      0          Graduate  No            5849            0.0000             nan         360.0000          1.0000           Urban         1          \n",
       "1  Male   Yes     1          Graduate  No            4583            1508.0000          128.0000    360.0000          1.0000           Rural         0          "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Graduate', 'Not Graduate'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Education.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_feats = df.copy() ##guardar para crear new features\n",
    "# new_feats.Ticket = df.Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dtype</th>\n",
       "      <th>Uniques</th>\n",
       "      <th>Nulls</th>\n",
       "      <th>% Nulls</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Loan_Status</th>\n",
       "      <td>int64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.8100</td>\n",
       "      <td>-1.3480</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>float64</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>-1.8820</td>\n",
       "      <td>1.5490</td>\n",
       "      <td>0.5620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <td>float64</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.4920</td>\n",
       "      <td>84.9560</td>\n",
       "      <td>-0.0590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanAmount</th>\n",
       "      <td>float64</td>\n",
       "      <td>203</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>2.6780</td>\n",
       "      <td>10.4020</td>\n",
       "      <td>-0.0370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>float64</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>-2.3620</td>\n",
       "      <td>6.6730</td>\n",
       "      <td>-0.0210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <td>int64</td>\n",
       "      <td>505</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.5400</td>\n",
       "      <td>60.5410</td>\n",
       "      <td>-0.0050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Dtype  Uniques  Nulls  % Nulls    Skew  Kurtosis  Correlation\n",
       "Loan_Status        int64    2        0     0.0000   -0.8100 -1.3480   1.0000      \n",
       "Credit_History     float64  2        50    0.0800   -1.8820 1.5490    0.5620      \n",
       "CoapplicantIncome  float64  287      0     0.0000   7.4920  84.9560   -0.0590     \n",
       "LoanAmount         float64  203      22    0.0400   2.6780  10.4020   -0.0370     \n",
       "Loan_Amount_Term   float64  10       14    0.0200   -2.3620 6.6730    -0.0210     \n",
       "ApplicantIncome    int64    505      0     0.0000   6.5400  60.5410   -0.0050     "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of numerical features (train dataset)\n",
    "data_summary(df = train.select_dtypes(include = \"number\"), target = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Summary:\n",
    "#Pclass and Fare as the most correlated features with Target\n",
    "#Fare has a huge Kurtosis and skew, so it must me processed.\n",
    "#Fare must have a significant amount of outliers to be analyzed\n",
    "#Parch and SibSp also must be transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.0000</td>\n",
       "      <td>614.0000</td>\n",
       "      <td>592.0000</td>\n",
       "      <td>600.0000</td>\n",
       "      <td>564.0000</td>\n",
       "      <td>614.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5403.4593</td>\n",
       "      <td>1621.2458</td>\n",
       "      <td>146.4122</td>\n",
       "      <td>342.0000</td>\n",
       "      <td>0.8422</td>\n",
       "      <td>0.6873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6109.0417</td>\n",
       "      <td>2926.2484</td>\n",
       "      <td>85.5873</td>\n",
       "      <td>65.1204</td>\n",
       "      <td>0.3649</td>\n",
       "      <td>0.4640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>150.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2877.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3812.5000</td>\n",
       "      <td>1188.5000</td>\n",
       "      <td>128.0000</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5795.0000</td>\n",
       "      <td>2297.2500</td>\n",
       "      <td>168.0000</td>\n",
       "      <td>360.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>81000.0000</td>\n",
       "      <td>41667.0000</td>\n",
       "      <td>700.0000</td>\n",
       "      <td>480.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  Loan_Status\n",
       "count 614.0000         614.0000           592.0000    600.0000          564.0000        614.0000    \n",
       "mean  5403.4593        1621.2458          146.4122    342.0000          0.8422          0.6873      \n",
       "std   6109.0417        2926.2484          85.5873     65.1204           0.3649          0.4640      \n",
       "min   150.0000         0.0000             9.0000      12.0000           0.0000          0.0000      \n",
       "25%   2877.5000        0.0000             100.0000    360.0000          1.0000          0.0000      \n",
       "50%   3812.5000        1188.5000          128.0000    360.0000          1.0000          1.0000      \n",
       "75%   5795.0000        2297.2500          168.0000    360.0000          1.0000          1.0000      \n",
       "max   81000.0000       41667.0000         700.0000    480.0000          1.0000          1.0000      "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Statistical summary of dataset (using train dataset)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observations:\n",
    "#- Only 38,4% of people survived\n",
    "#There are remarkable high values in the 3rd quantile of Fare to look into\n",
    "#75% of passengers declared without Parch\n",
    "#75% of passengers were 38 years old or less\n",
    "#There are different scales in the numerical data, so they will have to be rescaled later for modeling stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>79</td>\n",
       "      <td>8.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self_Employed</th>\n",
       "      <td>55</td>\n",
       "      <td>5.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanAmount</th>\n",
       "      <td>27</td>\n",
       "      <td>2.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>25</td>\n",
       "      <td>2.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>24</td>\n",
       "      <td>2.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>20</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Married</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total      %\n",
       "Credit_History    79    8.1000\n",
       "Self_Employed     55    5.6000\n",
       "LoanAmount        27    2.8000\n",
       "Dependents        25    2.5000\n",
       "Gender            24    2.4000\n",
       "Loan_Amount_Term  20    2.0000\n",
       "Married           3     0.3000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANALYSIS OF NULLS\n",
    "def nulls_summary(df):\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent_1 = df.isnull().sum()/df.isnull().count()*100\n",
    "    percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "    return missing_data[missing_data[\"Total\"] != 0]\n",
    "\n",
    "nulls_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>50</td>\n",
       "      <td>8.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self_Employed</th>\n",
       "      <td>32</td>\n",
       "      <td>5.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanAmount</th>\n",
       "      <td>22</td>\n",
       "      <td>3.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>15</td>\n",
       "      <td>2.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>14</td>\n",
       "      <td>2.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>13</td>\n",
       "      <td>2.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Married</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total      %\n",
       "Credit_History    50    8.1000\n",
       "Self_Employed     32    5.2000\n",
       "LoanAmount        22    3.6000\n",
       "Dependents        15    2.4000\n",
       "Loan_Amount_Term  14    2.3000\n",
       "Gender            13    2.1000\n",
       "Married           3     0.5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of nulls:\n",
    "\n",
    "#Nulls in train\n",
    "nulls_summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>29</td>\n",
       "      <td>7.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self_Employed</th>\n",
       "      <td>23</td>\n",
       "      <td>6.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>11</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>10</td>\n",
       "      <td>2.7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>6</td>\n",
       "      <td>1.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanAmount</th>\n",
       "      <td>5</td>\n",
       "      <td>1.4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total      %\n",
       "Credit_History    29    7.9000\n",
       "Self_Employed     23    6.3000\n",
       "Gender            11    3.0000\n",
       "Dependents        10    2.7000\n",
       "Loan_Amount_Term  6     1.6000\n",
       "LoanAmount        5     1.4000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nulls in test\n",
    "nulls_summary(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulls imputation for train and test using simple median imputation\n",
    "\n",
    "#Train nulls imputing\n",
    "\n",
    "def impute(df, target):\n",
    "    col = df.columns\n",
    "    if target == \"None\":\n",
    "        col = col.drop(target)\n",
    "    else:\n",
    "        pass\n",
    "    num_cols = df.select_dtypes(include = \"number\").columns\n",
    "    cat_cols = df.select_dtypes(include = \"object\").columns\n",
    "    #Impute numeric using median\n",
    "    for c in num_cols:\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "    #Impute cat using mode\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].fillna(df[c].mode()[0])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "train = impute(train, target = target)\n",
    "test = impute(test, target = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender               0\n",
       "Married              0\n",
       "Dependents           0\n",
       "Education            0\n",
       "Self_Employed        0\n",
       "ApplicantIncome      0\n",
       "CoapplicantIncome    0\n",
       "LoanAmount           0\n",
       "Loan_Amount_Term     0\n",
       "Credit_History       0\n",
       "Property_Area        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS1: CHANGING DTYPES OF SOME FEATS\n",
    "# #'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount','Loan_Amount_Term' to int64\n",
    "train[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
    "       'Loan_Amount_Term']] = train[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
    "       'Loan_Amount_Term']].astype(\"int64\")\n",
    "\n",
    "#Credit history to object like target\n",
    "train[[\"Credit_History\", \"Loan_Status\"]] = train[[\"Credit_History\", \"Loan_Status\"]].astype(\"object\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change same dtypes in test set\n",
    "test[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
    "       'Loan_Amount_Term']] = test[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
    "       'Loan_Amount_Term']].astype(\"int64\")\n",
    "\n",
    "#Credit history to object like target\n",
    "test[\"Credit_History\"] = test[\"Credit_History\"].astype(\"object\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Gender             614 non-null    object\n",
      " 1   Married            614 non-null    object\n",
      " 2   Dependents         614 non-null    object\n",
      " 3   Education          614 non-null    object\n",
      " 4   Self_Employed      614 non-null    object\n",
      " 5   ApplicantIncome    614 non-null    int64 \n",
      " 6   CoapplicantIncome  614 non-null    int64 \n",
      " 7   LoanAmount         614 non-null    int64 \n",
      " 8   Loan_Amount_Term   614 non-null    int64 \n",
      " 9   Credit_History     614 non-null    object\n",
      " 10  Property_Area      614 non-null    object\n",
      " 11  Loan_Status        614 non-null    object\n",
      "dtypes: int64(4), object(8)\n",
      "memory usage: 57.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BINNING\n",
    "num_cols = train.select_dtypes(include = \"number\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
       "       'Loan_Amount_Term'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.0000</td>\n",
       "      <td>614.0000</td>\n",
       "      <td>614.0000</td>\n",
       "      <td>614.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5403.4593</td>\n",
       "      <td>1621.2443</td>\n",
       "      <td>145.7524</td>\n",
       "      <td>342.4104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6109.0417</td>\n",
       "      <td>2926.2488</td>\n",
       "      <td>84.1072</td>\n",
       "      <td>64.4286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>150.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>12.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2877.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.2500</td>\n",
       "      <td>360.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3812.5000</td>\n",
       "      <td>1188.5000</td>\n",
       "      <td>128.0000</td>\n",
       "      <td>360.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5795.0000</td>\n",
       "      <td>2297.2500</td>\n",
       "      <td>164.7500</td>\n",
       "      <td>360.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>81000.0000</td>\n",
       "      <td>41667.0000</td>\n",
       "      <td>700.0000</td>\n",
       "      <td>480.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term\n",
       "count 614.0000         614.0000           614.0000    614.0000         \n",
       "mean  5403.4593        1621.2443          145.7524    342.4104         \n",
       "std   6109.0417        2926.2488          84.1072     64.4286          \n",
       "min   150.0000         0.0000             9.0000      12.0000          \n",
       "25%   2877.5000        0.0000             100.2500    360.0000         \n",
       "50%   3812.5000        1188.5000          128.0000    360.0000         \n",
       "75%   5795.0000        2297.2500          164.7500    360.0000         \n",
       "max   81000.0000       41667.0000         700.0000    480.0000         "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning the numerical features\n",
    "#ApplicantIncome\n",
    "bins = [-1, 3000, 5000, 90000]\n",
    "labels = [1,2,3]\n",
    "train['ApplicantIncome_bin'] = pd.cut(train['ApplicantIncome'], bins=bins, labels=labels)\n",
    "test['ApplicantIncome_bin'] = pd.cut(test['ApplicantIncome'], bins=bins, labels=labels)\n",
    "\n",
    "#CopplicantIncome\n",
    "bins = [-1, 1000, 2000, 42000]\n",
    "labels = [1,2,3]\n",
    "train['CoapplicantIncome_bin'] = pd.cut(train['CoapplicantIncome'], bins=bins, labels=labels)\n",
    "test['CoapplicantIncome_bin'] = pd.cut(test['CoapplicantIncome'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "#LoanAmount\n",
    "bins = [8,100,300, 750]\n",
    "labels = [1,2,3]\n",
    "train['LoanAmount_bin'] = pd.cut(train['LoanAmount'], bins=bins, labels=labels)\n",
    "test['LoanAmount_bin'] = pd.cut(test['LoanAmount'], bins=bins, labels=labels)\n",
    "\n",
    "#Loan_Amount_Term\n",
    "bins = [5,350,500]\n",
    "labels = [1,2]\n",
    "train['AmounTerm_bin'] = pd.cut(train['Loan_Amount_Term'], bins=bins, labels=labels)\n",
    "test[\"AmounTerm_bin\"] = pd.cut(test['Loan_Amount_Term'], bins=bins, labels=labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender                   0\n",
       "Married                  0\n",
       "Dependents               0\n",
       "Education                0\n",
       "Self_Employed            0\n",
       "ApplicantIncome          0\n",
       "CoapplicantIncome        0\n",
       "LoanAmount               0\n",
       "Loan_Amount_Term         0\n",
       "Credit_History           0\n",
       "Property_Area            0\n",
       "ApplicantIncome_bin      0\n",
       "CoapplicantIncome_bin    0\n",
       "LoanAmount_bin           0\n",
       "AmounTerm_bin            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>ApplicantIncome_bin</th>\n",
       "      <th>CoapplicantIncome_bin</th>\n",
       "      <th>LoanAmount_bin</th>\n",
       "      <th>AmounTerm_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Gender, Married, Dependents, Education, Self_Employed, ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History, Property_Area, ApplicantIncome_bin, CoapplicantIncome_bin, LoanAmount_bin, AmounTerm_bin]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.AmounTerm_bin.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sum(), test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Married_Yes</th>\n",
       "      <th>Dependents_1</th>\n",
       "      <th>Dependents_2</th>\n",
       "      <th>Dependents_3+</th>\n",
       "      <th>Education_Not Graduate</th>\n",
       "      <th>Self_Employed_Yes</th>\n",
       "      <th>Credit_History_1.0</th>\n",
       "      <th>Property_Area_Semiurban</th>\n",
       "      <th>Property_Area_Urban</th>\n",
       "      <th>ApplicantIncome_bin_2</th>\n",
       "      <th>ApplicantIncome_bin_3</th>\n",
       "      <th>CoapplicantIncome_bin_2</th>\n",
       "      <th>CoapplicantIncome_bin_3</th>\n",
       "      <th>LoanAmount_bin_2</th>\n",
       "      <th>LoanAmount_bin_3</th>\n",
       "      <th>AmounTerm_bin_2</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5849</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4583</td>\n",
       "      <td>1508</td>\n",
       "      <td>128</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  Gender_Male  Married_Yes  Dependents_1  Dependents_2  Dependents_3+  Education_Not Graduate  Self_Employed_Yes  Credit_History_1.0  Property_Area_Semiurban  Property_Area_Urban  ApplicantIncome_bin_2  ApplicantIncome_bin_3  CoapplicantIncome_bin_2  CoapplicantIncome_bin_3  LoanAmount_bin_2  LoanAmount_bin_3  AmounTerm_bin_2  Loan_Status\n",
       "0  5849             0                  128         360               1            0            0             0             0              0                       0                  1                   0                        1                    0                      1                      0                        0                        1                 0                 1                1          \n",
       "1  4583             1508               128         360               1            1            1             0             0              0                       0                  1                   0                        0                    1                      0                      1                        0                        1                 0                 1                0          "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding categorical features\n",
    "# #creating instance of one-hot-encoder\n",
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# # passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "\n",
    "t = train.drop(target, axis = 1)\n",
    "train= pd.concat((pd.get_dummies(t, drop_first = True), y_train), axis =1)\n",
    "test= pd.get_dummies(test, drop_first = True)\n",
    "\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.3127\n"
     ]
    }
   ],
   "source": [
    "###AUTOFEAT TOOLS TESTING\n",
    "from autofeat import AutoFeatClassifier\n",
    "t = train.copy()\n",
    "y = t.pop(target)\n",
    "X = t\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.3,random_state =0)\n",
    "model = AutoFeatClassifier()\n",
    "df = model.fit_transform(X, y)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Final Accuracy: %.4f\" % model.score(df, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 6 features after 5 feature selection runs\n",
      "[featsel] 6 features after correlation filtering\n",
      "[featsel] 4 features after noise filtering\n"
     ]
    }
   ],
   "source": [
    "from autofeat import FeatureSelector\n",
    "fsel = FeatureSelector(verbose=1)\n",
    "new_X = fsel.fit_transform(pd.DataFrame(X), pd.DataFrame(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Credit_History_1.0</th>\n",
       "      <th>Property_Area_Semiurban</th>\n",
       "      <th>Married_Yes</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Credit_History_1.0  Property_Area_Semiurban  Married_Yes  CoapplicantIncome\n",
       "0  1                   0                        0            0                \n",
       "1  1                   0                        1            1508             \n",
       "2  1                   0                        1            0                \n",
       "3  1                   0                        1            2358             \n",
       "4  1                   0                        0            0                "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####CORRELATION AND FEATURE SELECTION ANALYSIS\n",
    "class Selector:\n",
    "    def __init__(self):\n",
    "        # self.target = target\n",
    "        # self.df = df\n",
    "        pass\n",
    "        \n",
    "    # def rescale(self, df, target):\n",
    "    #     #Rescale data if necessary\n",
    "    #     X2 = df.copy()\n",
    "    #     y2 = X2.pop(target)\n",
    "    #     scaler = StandardScaler().fit(X2)\n",
    "    #     XRescaled = scaler.transform(X2)\n",
    "    #     X_rescaled = pd.DataFrame(XRescaled, columns = X2.columns)\n",
    "    #     print(\"1. Data rescaling:\")\n",
    "    #     print(\"Data has been rescaled\")\n",
    "    #     return pd.concat((y2, X_rescaled), axis = 1)\n",
    "\n",
    "    def variance_selector(self, df, target):\n",
    "        #Rescale data if necessary\n",
    "        print(\"1.Variance Threshold feature selection:\")\n",
    "        print(\"\")\n",
    "        X2 = df.copy()\n",
    "        y2 = X2.pop(target)\n",
    "        print(\"Initial features:\", X2.shape[1])\n",
    "        low = [col for col in X2.columns if X2[col].std() < 0.5]\n",
    "        # print(\"Estos son antes de escale:\", low)\n",
    "        # scaler = StandardScaler().fit(X2)\n",
    "        # XRescaled = scaler.transform(X2)\n",
    "        # X_rescaled = pd.DataFrame(XRescaled, columns = X2.columns)\n",
    "        # low = [col for col in X2.columns if X[col].std() < 0.5]\n",
    "        #Analysis of amount of variation and droping all features with low variance\n",
    "        var_tresh = VarianceThreshold(threshold = 0.5)\n",
    "        var_tresh.fit_transform(X2)\n",
    "        data_transformed = X2.loc[:, var_tresh.get_support()]\n",
    "        #print(\"Selected feat:\", data_transformed.columns)\n",
    "        print(\"Removed features:\", set(X2.columns) - set(data_transformed.columns))\n",
    "        print(\"Final features:\", data_transformed.shape[1])\n",
    "        print(\"{} features with low variance removed\".format(X2.shape[1] - data_transformed.shape[1]))\n",
    "        #Rejoin\n",
    "        df = pd.concat((y2, data_transformed), axis = 1)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def single_value_dominate(self, df, target):\n",
    "        print(\"2.Removing columns with single value dominating > 95%:\")\n",
    "        print(\"\")\n",
    "        X2 = df.copy()\n",
    "        y2 = X2.pop(target)\n",
    "        remove_cols = []\n",
    "        for col in X2.columns:\n",
    "            count= 0\n",
    "            count = sum([+1 for i in X2[col].values if i == X2[col].mode()[0]])\n",
    "            if count/X2[col].shape[0] >= 0.95:\n",
    "                remove_cols.append(col)\n",
    "        selected_cols = set(df.columns) - set(remove_cols)\n",
    "        print(\"Removed cols:\", len(remove_cols))\n",
    "        print(\"Total selected cols:\", len(selected_cols))\n",
    "        df = df[selected_cols]\n",
    "        return df\n",
    "\n",
    "\n",
    "    def calcDrop(self, res):\n",
    "        # All variables with correlation > cutoff\n",
    "        all_corr_vars = list(set(res['v1'].tolist() + res['v2'].tolist()))\n",
    "        \n",
    "        # All unique variables in drop column\n",
    "        poss_drop = list(set(res['drop'].tolist()))\n",
    "\n",
    "        # Keep any variable not in drop column\n",
    "        keep = list(set(all_corr_vars).difference(set(poss_drop)))\n",
    "        \n",
    "        # Drop any variables in same row as a keep variable\n",
    "        p = res[ res['v1'].isin(keep)  | res['v2'].isin(keep) ][['v1', 'v2']]\n",
    "        q = list(set(p['v1'].tolist() + p['v2'].tolist()))\n",
    "        drop = (list(set(q).difference(set(keep))))\n",
    "\n",
    "        # Remove drop variables from possible drop \n",
    "        poss_drop = list(set(poss_drop).difference(set(drop)))\n",
    "        \n",
    "        # subset res dataframe to include possible drop pairs\n",
    "        m = res[ res['v1'].isin(poss_drop)  | res['v2'].isin(poss_drop) ][['v1', 'v2','drop']]\n",
    "            \n",
    "        # remove rows that are decided (drop), take set and add to drops\n",
    "        more_drop = set(list(m[~m['v1'].isin(drop) & ~m['v2'].isin(drop)]['drop']))\n",
    "        for item in more_drop:\n",
    "            drop.append(item)\n",
    "            \n",
    "        return drop\n",
    "\n",
    "    def corrX_new(self, df, target, cut) :\n",
    "        print(\"\")\n",
    "        print(\"3. Removing features with high pairwise correlation\")\n",
    "        # Get correlation matrix and upper triagle\n",
    "        df2 = df.drop(target, axis = 1)\n",
    "\n",
    "        corr_mtx = df2.corr().abs()\n",
    "        avg_corr = corr_mtx.mean(axis = 1)\n",
    "        up = corr_mtx.where(np.triu(np.ones(corr_mtx.shape), k=1).astype(np.bool))\n",
    "\n",
    "        dropcols = list()\n",
    "\n",
    "        res = pd.DataFrame(columns=(['v1', 'v2', 'v1.target', \n",
    "                                        'v2.target','corr', 'drop' ]))\n",
    "\n",
    "        for row in range(len(up)-1):\n",
    "            col_idx = row + 1\n",
    "            for col in range (col_idx, len(up)):\n",
    "                if(corr_mtx.iloc[row, col] > cut):\n",
    "                    if(avg_corr.iloc[row] > avg_corr.iloc[col]): \n",
    "                        dropcols.append(row)\n",
    "                        drop = corr_mtx.columns[row]\n",
    "                    else: \n",
    "                        dropcols.append(col)\n",
    "                        drop = corr_mtx.columns[col]\n",
    "                    \n",
    "                    s = pd.Series([ corr_mtx.index[row],\n",
    "                    up.columns[col],\n",
    "                    avg_corr[row],\n",
    "                    avg_corr[col],\n",
    "                    up.iloc[row,col],\n",
    "                    drop],\n",
    "                    index = res.columns)\n",
    "            \n",
    "                    res = res.append(s, ignore_index = True)\n",
    "\n",
    "        dropcols_names = self.calcDrop(res)\n",
    "        print(\"{} features removed\".format(len(dropcols_names)))\n",
    "        print(\"Features removed:\", dropcols_names)\n",
    "        print(\"\")\n",
    "        print(\"Selected features:\", df.shape[1] - len(dropcols_names))\n",
    "        selected_cols = set(df.columns) - set(dropcols_names)\n",
    "        print(\"\")\n",
    "        print(\"Selected:\", selected_cols)\n",
    "        return df[selected_cols]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def calculate_vif_(self, X, thresh=5.0):\n",
    "        cols = X.columns\n",
    "        variables = np.arange(X.shape[1])\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            dropped=False\n",
    "            c = X[cols[variables]].values\n",
    "            vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "\n",
    "            maxloc = vif.index(max(vif))\n",
    "            if max(vif) > thresh:\n",
    "                print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "                variables = np.delete(variables, maxloc)\n",
    "                dropped=True\n",
    "\n",
    "        print('Remaining variables:')\n",
    "        print(X.columns[variables])\n",
    "        return X[cols[variables]]\n",
    "\n",
    "\n",
    "    def corr_target(self, df):\n",
    "        print(\"\")\n",
    "        print(\"Remove features with low target correlation:\")\n",
    "        #Removes all feats with correlation under threshold\n",
    "        remove_features = [feat for feat in df.columns if df.corr().abs()[[target]].loc[feat, :][0] < 0.05]\n",
    "        selected_cols = set(df.columns) - set(remove_features)\n",
    "        print(\"Total features removed:\", len(remove_features))\n",
    "        print(\"Final features selected:\", len(selected_cols))\n",
    "\n",
    "        return df[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Removing features with high pairwise correlation\n",
      "2 features removed\n",
      "Features removed: ['LoanAmount', 'AmounTerm_bin_2']\n",
      "\n",
      "Selected features: 20\n",
      "\n",
      "Selected: {'ApplicantIncome', 'ApplicantIncome_bin_3', 'CoapplicantIncome_bin_3', 'Loan_Status', 'LoanAmount_bin_2', 'Property_Area_Urban', 'Credit_History_1.0', 'Married_Yes', 'Loan_Amount_Term', 'Education_Not Graduate', 'Dependents_1', 'Self_Employed_Yes', 'Property_Area_Semiurban', 'Dependents_3+', 'LoanAmount_bin_3', 'ApplicantIncome_bin_2', 'Dependents_2', 'Gender_Male', 'CoapplicantIncome_bin_2', 'CoapplicantIncome'}\n",
      "\n",
      "Remove features with low target correlation:\n",
      "Total features removed: 13\n",
      "Final features selected: 7\n"
     ]
    }
   ],
   "source": [
    "##Remove the most correlated feats and target\n",
    "scl = Selector()\n",
    "df_corr = scl.corrX_new(df = train, target = \"Loan_Status\", cut = 0.6)\n",
    "df_target =scl.corr_target(df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: Index(['Loan_Status', 'Property_Area_Semiurban', 'Credit_History_1.0',\n",
      "       'Married_Yes', 'Dependents_2', 'CoapplicantIncome',\n",
      "       'Education_Not Graduate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Actualizamos para testing\n",
    "train = train[df_target.columns]\n",
    "test_cols = set(train.columns)- set([\"Loan_Status\"])\n",
    "test = test[test_cols]\n",
    "\n",
    "print(\"Selected features:\", train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Credit_History_1.0\n",
    "# Property_Area_Semiurban\n",
    "# Married_Yes\n",
    "# CoapplicantIncome\n",
    "\n",
    "\n",
    "###FEATURE ENGINEERING 2021\n",
    "#Data transformations\n",
    "#Binarization\n",
    "#Rescaling (done)\n",
    "\n",
    "all_data = pd.concat((train.drop(target, axis = 1), test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data trsnformation\n",
    "###treatment of skewed features\n",
    "def feat_transform(all_data):\n",
    "    cols = all_data.columns\n",
    "    #Seleccionamos los numeric features only (usamos cols porque esos son los feats que quedaron)\n",
    "    numeric_features = all_data.loc[:, list(cols)].dtypes[(all_data.dtypes != \"category\") & (all_data.dtypes != \"uint8\")].index\n",
    "\n",
    "    # #Calculamos los skewes y los ordenamos...\n",
    "    skewed_features = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending =False)\n",
    "\n",
    "    #Seleccionamos los que tengan un abs(skew) >0.7\n",
    "    skewed_max = pd.DataFrame({\"Skew\": skewed_features})\n",
    "    skewed_max  = skewed_max[abs(skewed_max[\"Skew\"]) > 0.7]\n",
    "    skewed_max.dropna()\n",
    "\n",
    "    #Aplicamos ahora la BoxCox transformation\n",
    "    l_opt = {}\n",
    "    for f in skewed_max.index:\n",
    "        all_data[f], l_opt[f] = boxcox((all_data[f] + 1))\n",
    "\n",
    "    #Checking the results...\n",
    "\n",
    "    skewed_features_2 = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending =False)\n",
    "    sk_2 = pd.DataFrame({\"Skew_before\":skewed_features, \"Skew_after\": skewed_features_2})\n",
    "    sk_2 = sk_2[abs(sk_2[\"Skew_before\"]) > 0.7].sort_values(by = \"Skew_before\", ascending = False)\n",
    "    print(sk_2)\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Skew_before  Skew_after\n",
      "CoapplicantIncome 7.4732       -0.1453    \n",
      "                   Skew_before  Skew_after\n",
      "CoapplicantIncome 4.2399       -0.2006    \n"
     ]
    }
   ],
   "source": [
    "#Transform train\n",
    "train = pd.concat((feat_transform(train.loc[:, train.columns != target]), train[target]), axis = 1)\n",
    "\n",
    "#transform test\n",
    "test = feat_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dtype</th>\n",
       "      <th>Uniques</th>\n",
       "      <th>Nulls</th>\n",
       "      <th>% Nulls</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Loan_Status</th>\n",
       "      <td>int64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.8100</td>\n",
       "      <td>-1.3480</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_History_1.0</th>\n",
       "      <td>uint8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.0220</td>\n",
       "      <td>2.0950</td>\n",
       "      <td>0.5410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Property_Area_Semiurban</th>\n",
       "      <td>uint8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>-1.7580</td>\n",
       "      <td>0.1370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Married_Yes</th>\n",
       "      <td>uint8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.6450</td>\n",
       "      <td>-1.5890</td>\n",
       "      <td>0.0910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education_Not Graduate</th>\n",
       "      <td>uint8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3680</td>\n",
       "      <td>-0.1300</td>\n",
       "      <td>-0.0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents_2</th>\n",
       "      <td>uint8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.8140</td>\n",
       "      <td>1.2960</td>\n",
       "      <td>0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <td>float64</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.1460</td>\n",
       "      <td>-1.8780</td>\n",
       "      <td>0.0570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Dtype  Uniques  Nulls  % Nulls    Skew  Kurtosis  Correlation\n",
       "Loan_Status              int64    2        0     0.0000   -0.8100 -1.3480   1.0000      \n",
       "Credit_History_1.0       uint8    2        0     0.0000   -2.0220 2.0950    0.5410      \n",
       "Property_Area_Semiurban  uint8    2        0     0.0000   0.4980  -1.7580   0.1370      \n",
       "Married_Yes              uint8    2        0     0.0000   -0.6450 -1.5890   0.0910      \n",
       "Education_Not Graduate   uint8    2        0     0.0000   1.3680  -0.1300   -0.0860     \n",
       "Dependents_2             uint8    2        0     0.0000   1.8140  1.2960    0.0620      \n",
       "CoapplicantIncome        float64  287      0     0.0000   -0.1460 -1.8780   0.0570      "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_summary(df = train, target = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####   ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has been rescaled...\n",
      "test data has been rescaled...\n"
     ]
    }
   ],
   "source": [
    "##rescale end send to testing\n",
    "\n",
    "def rescaler(df, target):\n",
    "#Rescaling utility\n",
    "    if target !=\"None\":\n",
    "        y = df.pop(target)\n",
    "        X = df\n",
    "        test_ID = y.index\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        XRescaled = scaler.transform(X)\n",
    "        df_rescaled = pd.DataFrame(XRescaled, columns = X.columns)\n",
    "        df = pd.concat((y, df_rescaled), axis = 1)\n",
    "        print(\"train data has been rescaled...\")\n",
    "    else:\n",
    "        \n",
    "        scaler = StandardScaler().fit(df)\n",
    "        XRescaled = scaler.transform(df)\n",
    "        df = pd.DataFrame(XRescaled, columns = df.columns)\n",
    "        print(\"test data has been rescaled...\")\n",
    "    return df\n",
    "\n",
    "train = rescaler(train, target)\n",
    "test = rescaler(test,target = \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAHiCAYAAABP+3CeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABAIklEQVR4nO3de3zP9f//8ft7B6QNa0wOnVTmfCgxxxjFMLNJCkMJSdEnZmREkeMcV0n5FNJXYoxWDhGpmApJDqmQCRub2djs8H79/vDz/rSYDdtzptv1cunS+/06PJ+P12NLd6/na3vbLMuyBAAAAGOcCrsAAACAfxsCGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAANQJI0fP14BAQEKCAhQrVq11LZtW8f7tLS0fJlj//79atasWbZtixYtUtOmTR1zde/e/bLzTp48qaeeeipfarhWzz77rBISEq77/N27d2vMmDFX3PfNN9+oVatWeuKJJ667x2FhYdqzZ8911wfcKlwKuwAAuB5hYWGO176+vpo2bZpq166dL2NnZmbqo48+0nvvvafz589n27dz506NGDFC/v7+OZ5fvnx5LVmyJF9quVbffvvtDZ3/22+/6eTJk1fcFx0dra5du+qFF1647vG/++47devW7brPB24VBDAAt5y33npL0dHRcnZ21n333afRo0erXLlyCg4OVo0aNfTjjz8qMTFRAQEBGjx48GXn7927VwcOHFBERISeffbZbPt27typlJQUzZs3T15eXho+fLi8vb2zHRMbGyt/f3/t3LlTc+bM0Z9//qmTJ08qPj5eNWvWVKNGjbRy5UrFxsYqJCREHTt21Jw5c3TkyBGdOHFC8fHxqlatmiZMmCA3NzcdPHhQr7/+us6cOSObzaZnn31WnTt3VkxMjCZMmKCSJUvq3LlzqlWrliSpd+/emjdvnvbv3693331X6enpSkhIUOfOnfXyyy8rJiZGM2bM0F133aWDBw8qMzNT48aNU8WKFTV79mwlJydr5MiRmjhxouOa3n//fW3YsEHFixdXcnKyQkND9c4772jdunWy2+2qVKmSXnvtNZUvX167du3S1KlTlZ6ervj4eDVp0kRvvvmmZsyYobi4OA0bNkxTpkzRtGnT1KNHD7Vr106SFBwc7Hhfq1YttW7dWvv379e0adNUsmRJTZgwQWfOnFFWVpaCg4P1xBNP6Ny5cxo5cqSOHDkiJycn1axZU6+//rqcnFjgwU3OAoAirlWrVtbu3bsty7KsZcuWWd26dbPOnTtnWZZlzZ4923r22Wcty7Ksnj17Wv369bPS09OtpKQkq23bttbGjRtzHPfo0aNWvXr1HO/PnTtnPfvss9b27dsty7Ks6Ohoq3nz5lZKSkqO582ePdtq1aqVdfbsWSs1NdV65JFHrIkTJ1qWZVnr16+3Hn/8ccdxLVq0sOLj462srCzrlVdesSZNmmRlZGRYrVu3ttauXWtZlmWdOHHCat68ubVjxw5r27ZtVrVq1azY2FjH3FWrVrVOnz5t2e12q2fPntahQ4cc51WvXt06ffq0tW3bNqt69erW3r17LcuyrPnz51s9evSwLMuyli9fbvXv3/+K/QgNDbXef/99y7Isa8WKFdbLL79sZWRkWJZlWUuWLLGee+45y7Is6z//+Y+1bds2y7IsKyUlxWrUqJH1888/X/a16tmzp/XFF184xv/7+6pVq1orVqywLMuyMjIyrPbt21t79uyxLMuyzp49a/n5+Vk7d+60VqxY4fj6ZmZmWqNGjbIOHz6cw1cUuHlwBwzALeXrr79WUFCQSpYsKUnq1auX5s6dq/T0dElSt27d5OrqKldXV7Vr187xXFNelCxZUvPnz3e8b9++vd555x39/PPP8vHxyfG8Jk2ayN3dXZLk5eWl5s2bS5LuvvtunTlzxnFcu3btVLZsWUnSE088oTfffFNdunTRhQsX9Pjjj0u6uLz5+OOPa8uWLWrUqJEqVKigSpUqXTanzWbT3LlztWnTJn322Wf6/fffZVmWUlNTJUkVK1ZU9erVJUk1atTQihUr8tSDS7766iv9/PPP6tKliyTJbrc7xp40aZK+/vprzZ07V3/88YcuXLhw2VJuXjRo0ECSdPjwYf3555969dVXHfvS0tK0d+9eNW/eXDNmzFBwcLCaNGmi3r1765577rnmuQDTCGAAbil2u102my3b+8zMTMd7F5f//bFnWdY1LVUdO3ZMGzduVHBwcLYx/j7mlRQrVizb+5yOd3Z2zla3k5OTsrKysl3PpTkvXdOloPlP58+fV2BgoNq0aaMGDRqoS5cu+vLLL2X9/4//LVGihONYm83m2J5Xdrtdzz33nOOHENLT05WUlCRJ6tmzp7y9vdW8eXP5+fnpp59+ynH8v2/PyMjItu/StWVlZcnd3V1RUVGOfadOnZK7u7uKFy+u9evXKyYmRtu2bdMzzzyj119/Xb6+vtd0PYBpLJIDuKU0b95cy5cvd9xxWbRokR555BFHCFq1apXsdruSkpL0xRdfXNP/qG+77TbNnDlTu3fvliRt3rxZqampqlOnTr7UvmHDBiUnJ8tut2vp0qVq1aqVqlSpIhcXF61bt07SxZ+wXLt2rZo0aXLFMZydnZWZmakjR44oJSVFL7/8snx9fRUTE6P09HTZ7far1nDp/Nw0a9ZMy5YtU0pKiiRp1qxZGj58uM6ePauff/5Zw4YN0+OPP64TJ07ozz//dMz79/HvuOMOx09E/vbbbzpw4MAV57rvvvtUokQJRwA7fvy4OnbsqD179ujjjz/WyJEj1axZM4WEhKhZs2bau3dvrvUDhY07YABuKU888YSOHz+url27ym6365577tG0adMc+9PS0hwPb3fv3l2NGzfO89h33HGHZs6cqTFjxigjI0Nubm566623LrvDdb3Kli2rfv36KTExUY888oief/55ubq66u2339b48eM1Z84cZWVladCgQfLx8VFMTMxlY7Rr107BwcGaNWuWWrZsKT8/PxUrVkxVq1bVAw88oCNHjly13nr16umtt97Siy++qIiIiByP69q1q06ePKknn3xSNptNFSpU0KRJk1SqVCn1799fgYGBKlmypMqXL6+HHnpIR44cUePGjfXYY48pJCREY8eO1cCBAzVixAht3rxZVapUcSw5/lOxYsX09ttva8KECXr//feVmZmpIUOG6OGHH1b16tW1fft2tW/fXrfddpsqVKiQ7Q4lcLOyWdd63xkAiqi//5TdzWbOnDlKTEzM8XdwAbi1sAQJAABgGHfAAAAADOMOGAAAgGEEMAAAAMMIYAAAAIbxayiQ7xITz8lu59HCguLp6abTp1MKu4xbGj0uePTYDPpcsJycbPLwuP26ziWAId/Z7RYBrIDR34JHjwsePTaDPt+cWIIEAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABjmUtgF4Nbj6emWb2OlXchU8tnUfBsPAICbAQEM+a7v+HWKS8yf0LQ6PEDJ+TISAAA3D5YgAQAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAtg/xMbGqlatWgoICFBAQIDatm2rkSNH6tSpU4VSi6+v73Wde/ToUb366qvXde7vv/+uHj16KCAgQN26ddO+ffuuaxwAAHBlBLAr8PLyUlRUlKKiorRmzRqVLVtWgwcPLuyyrslff/2lo0ePXte5YWFh6tevn6KiovTyyy8rNDQ0n6sDAODfzaWwC7jZ2Ww2vfTSS2ratKn279+vr7/+Wl988YWysrLUrFkzhYSE6NixYxo4cKCqVKmi3377TRUrVtTUqVNVpkwZff3115o9e7YyMzNVuXJlvfHGG/Lw8JCvr686deqkb775RqmpqZo8ebJq1aqlvXv3atSoUZKkatWqOeo4deqUxowZoxMnTshms2no0KFq0qSJ5syZo5MnT+rIkSM6duyYunbtqoEDB2r8+PGKjY3VuHHjNGDAAA0bNkznz5+Xk5OTwsLCVK9evRyvuWvXrmrevLkkydvbW8ePHy/QHgMA8G9DAMuDYsWK6Z577tH+/fu1Z88eLVu2TDabTSEhIVq1apUefvhh/frrrwoLC1OjRo00adIkRURE6IUXXlB4eLgWLlyo0qVLa8mSJZo2bZomTJggSSpTpoyWLVumRYsW6d1339WcOXMUGhqqESNGqGnTpnrrrbcUExMjSZowYYK6dOmi1q1bKy4uTt27d9fKlSslSQcOHNDixYuVnJysNm3aqEePHgoLC1NERIRee+01RUREqGXLlnruuef09ddf68cff7xqAAsKCnK8nj17ttq0aVNgvc2LcuXcC3X+mxE9KXj0uODRYzPo882JAJZHNptNCxcuVEJCgiOgpKWlqWLFinr44Yd17733qlGjRpKkzp07a9iwYWratKmOHz+uXr16SZLsdrtKly7tGPPSXaYHH3xQ69atU0JCguLi4tS0aVNJF4PQ8uXLJUnfffed/vjjD82ePVuSlJmZ6VhibNSokYoVKyZPT0+VKVNGycnJ2Wpv3LixXnrpJe3bt0+PPvqoevbsmev1WpalKVOm6KefftLChQuvu2/5IT4+OfeD/kXKlXOnJwWMHhc8emwGfS5YTk42eXq6Xde5BLA8SE9P16FDh9SoUSP5+/vrmWeekSSdPXtWzs7OSkxMlIvL/1ppWZacnZ2VlZWlhx56SHPnzpUkXbhwQefOnXMcV7x4cUkXw92lf1uW5djv7OzseG2327VgwQKVKVNGkhQXFydPT099+eWXjnGuNIYkPfzww4qOjtamTZv0+eefa8WKFfrggw9yvN7MzEyFhobq5MmTWrhwodzd+dsTAAD5iYfwc2G32zVnzhzVrVtXXbp0UVRUlM6dO6fMzEwNGjRIa9eulSQdOnTI8dOCy5cvV4sWLVS3bl3t2rVLhw4dkiS9/fbbmjJlSo5zeXh4qGLFitq0aZMk6bPPPnPs8/Hx0ccffyxJ+u233+Tv76/U1NQcx3J2dlZmZqYkacqUKVq1apUCAwM1ZswY7d2796rXPHnyZKWkpOi///0v4QsAgALAHbAriIuLU0BAgKSLAax69eqaPn26Spcurf379+vJJ59UVlaWmjdvrsDAQB07dkylS5fW7Nmz9eeff8rb21vjx49XyZIl9eabb+rll1+W3W5X+fLlNXXq1KvOPXXqVI0cOVIzZ87M9pxWWFiYxowZI39/f0kXQ5WbW863Pe+//34lJycrJCREr7zyioYOHarIyEg5Oztr8uTJOZ6XkJCgxYsXq3Llyuratatje1RUVF5aBwAA8sBm/XO9CtcsNjZWvXr10saNGwu7lJtC3/HrFJeY8925a7E6PIDnF/6BZzoKHj0uePTYDPpcsHgGDNds6NCh+u233y7b7uvrqyFDhhRCRQAA/HsQwPJB5cqVi9zdr/Dw8MIuAQCAfy0ewgcAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGB9FhHw3P+zxfBsr7UJmvo0FAMDNggCGfHf6dIrsdquwywAA4KbFEiQAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAw1wKuwDcejw93fJ1vLQLmUo+m5qvYwIAUJgIYMh3fcevU1xi/gWm1eEBSs630QAAKHwsQQIAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAw/IlgMXGxqpWrVoKCAjI9s/ixYsvOzYyMlIjRozIj2kdgoODHa8DAgLydWxJ8vb21qeffnrZnDExMVc9b+TIkTp27NgV923dulU9e/ZU27Zt9dhjj2nw4ME6ceLEDdWZl5pycrVaAQBA/nLJr4G8vLwUFRWVX8Ndk+3btzteF1QN4eHhatasmSpUqJDnc2JiYjRo0KDLtv/www8KCQlRRESE6tWrJ0lavHixBg0apOXLl+dXydckp1oBAED+y7cAdjUrV67UO++8Izc3N1WqVEklS5aUJPn6+mrhwoWqXLmyYmJiFBERoUWLFmnfvn0aM2aM0tLSVLp0aU2bNk1ly5bV2LFjdfDgQZ06dUre3t6aPn26pk2bJknq2rWrPv30U3l7e+vAgQNKTU1VWFiYDhw4IJvNpr59+6pz586KjIzUli1blJSUpKNHj6pp06YaO3ZsrtfQu3dvhYWFaf78+ZftW758uT744APZbDbVrFlTo0eP1uLFixUXF6f+/ftr8eLF8vDwcBz/9ttva+DAgY7wJUk9evRQWlqa0tPTtXPnTk2dOlV2u10PPvigXnnlFb366qtKTk5WXFycAgMDNWTIEKWnp2vUqFHas2ePKlWqpMTEREnK1ktJGjFihBo2bKigoCDNmDFDW7duVVJSkry8vDRjxgxFRkZmq/Xo0aOaOHGi0tLS5OHhoXHjxumuu+663i9/vihXzr1Q57/Z0I+CR48LHj02gz7fnPItgMXFxV22/DdlyhSVKVNG06ZN08qVK1WmTBkNGDDAEcByMmzYMA0bNkytWrXSxx9/rAULFsjX11eurq765JNPZLfb1bt3b23evFlhYWFatGjRZUuEc+bMkYeHhz777DMlJCSoa9euqlatmiRp586d+uyzz+Ts7Kx27drp6aeflre391Vr6tevn9avX69PP/1UXbt2dWw/cOCA5s6dq6VLlzrCSkREhEJDQ7VkyRLNmzcvW/iSpF27dl1xGbZv376O14cPH9ZXX30ld3d3zZ8/Xx07dlRgYKCSk5P16KOPKjg4WCtWrJAkffHFFzp8+LA6dep01Ws4cuSI/vjjDy1ZskROTk4aPny4Vq1apf79+ztqvf322xUWFqa5c+eqYsWK2rJli0aPHq0PP/zwqmMXtPj45EKd/2ZSrpw7/Shg9Ljg0WMz6HPBcnKyydPT7brOLfAlyDVr1qh+/foqW7asJMnf31/btm3LcZyEhATFx8erVatWkqTu3bs79pUpU0aLFy/WH3/8ocOHD+v8+fM5jrNt2za9+eabkqQ77rhDrVu31vbt2+Xm5qb69evLze1iw+666y4lJSXlen0uLi6aNGmSevXqpWbNmjm2f//992rVqpUjZHXr1k0jR47MdTybzSZJSk9PdwS6pKQkTZ8+XZJ03333yd394t9a+vbtq23btmn+/Pk6ePCgMjIylJqaqu3bt6tbt26SpHvvvVf169e/6pz33HOPQkND9emnn+rQoUPatWuX7r777mzHHD58WEePHtXAgQMd21JSUnK9HgAAkHcFvgRps9lkWdb/JnTJPuWlfZmZmZIkV1dXRziRpAsXLiguLk6//vqrZs+erV69eikoKEiJiYnZxv2nf+6zLEtZWVmSpOLFi+dY39VUrVrVsRR5id1uv2yeS9eSk9q1a2vHjh168MEHVaxYMUdwDQ4OVkZGhiSpRIkSjuMnTZqko0ePqmPHjmrTpo2+++47WZaVY2//uf3SmHv27NHQoUPVp08ftW3bVk5OTpddu91uV+XKlR01ZWVl6dSpU3nqDwAAyJsC/zUUDz/8sHbt2qWTJ0/Kbrfr888/d+zz8PDQb7/9JknasGGDJMnd3V3ly5fXN998I+niQ/WzZs3S1q1b5efnpy5duqhUqVKKiYlxBCpnZ+fLQo+Pj4+WLVsm6eJdtQ0bNqhhw4Y3fD39+vVTYmKidu7cKUlq2LChNm7cqDNnzkiSli5dqkaNGjnqulTj37300kt666239NNPPzm27d+/X0ePHpWzs/Nlx3/77bfq27ev/Pz8dOjQIUcvGzdurNWrV8tut+vYsWPasWOHpIt9PXr0qC5cuKAzZ87oxx9/lHTxbl3Dhg319NNP695779WmTZuy9TArK0tVqlRRUlKSfvjhB0kXn28bNmzYDfcNAAD8T4E+A/bII48oLCxMYWFh6tOnj2677TY98MADjv2DBw/WG2+8oYiIiGzLelOnTtXYsWM1depUeXh4aMqUKUpMTNSwYcMUHR0tV1dXPfTQQ4qNjZUktW7dWgEBAYqMjHSMMWjQII0dO1b+/v7KysrS888/r5o1a+rAgQM3dJ2XliKDgoIkSdWqVdOAAQMcd69q1qypcePGSZJatmyp/v376/3338/2EHuDBg00Y8YMzZw5U6dOndL58+dVoUIFhYaGqkGDBpf9KokBAwZo+PDhKlGihO68807VqlVLsbGx6t69uw4ePCg/Pz9VqlRJVatWlSQ9+OCDevTRR9WhQwdVqlRJDz/8sCSpffv2evHFF+Xv7y9JjnH+WeusWbM0YcIEXbhwQW5ubpo8efIN9QwAAGRns/K6/gbkUd/x6xSXmJpv460OD+Ah0r/hodqCR48LHj02gz4XrJviIfyi7M8//9RLL710xX3jx49X7dq1DVcEAABuZQQwSXfffXeh/RJZAADw78NnQQIAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYxm/CR76bH/Z4vo6XdiEzX8cDAKCwEcCQ706fTpHdzme8AwCQE5YgAQAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADDMpbALwK3H09PN2FxpFzKVfDbV2HwAAOQHAhjyXd/x6xSXaCYUrQ4PULKRmQAAyD8sQQIAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQSw6xQbGytvb2+NGTMm2/Z9+/bJ29tbkZGR1zXuhg0bNGvWrGuqw9fXN8f9mZmZ6ty5sz766KNs28PDwzV48ODrqhEAANwYl8IuoCgrU6aMtmzZoqysLDk7O0uSPv/8c91xxx3XPWbr1q3VunXr/CpRLi4umjhxovr06aM2bdrozjvv1P79+xUVFaWVK1fm2zwAACDvCGA34Pbbb1e1atX0/fffy8fHR5L07bffqkmTJpKkjz76SFFRUUpNTZWrq6vCw8NVpUoV+fr6qk6dOtq3b5+mTp2q4cOHy8PDQyVKlJC/v7+2b9+uSZMmaffu3Zo4caLS0tLk4eGhcePG6a677tLevXs1atQoSVK1atVyrbN69ep6+umnNX78eM2ePVthYWEaM2aM7rjjDq1cuVILFiyQ3W5XzZo19dprr8nJyUmvvvqqDh48KEnq3r27nnzyyQLqIgAA/z4EsBvk5+entWvXysfHR7t375a3t7csy1JKSoo2btyoRYsWqUSJEpo1a5YWL16s0aNHS5JatGihmTNnKjY2VocOHdL777+vypUrO5Yu09PTFRYWprlz56pixYrasmWLRo8erQ8//FChoaEaMWKEmjZtqrfeeksxMTG51vnCCy8oKChIISEhuv/++9WmTRsdPHhQS5cu1ZIlS1S8eHGFh4dr/vz5atCggZKSkrRy5UqdPHlS4eHhN3UAK1fOvbBLMO7feM2m0eOCR4/NoM83JwLYDfL19dXMmTNlt9v1xRdfyM/PT59//rnc3NwUHh6u6OhoHT58WFu2bFH16tUd59WtW9fx2tPTU5UrV8427uHDh3X06FENHDjQsS0lJUUJCQmKi4tT06ZNJUlBQUFavnx5rnUWK1ZMY8aM0QsvvKCvvvpKkhQTE6MjR444wlVGRoZq1Kihp59+WocOHVLfvn3VokULDR8+/PobZEB8fHJhl2BUuXLu/7prNo0eFzx6bAZ9LlhOTjZ5erpd17kEsBt0aRnyxx9/1LZt2zR06FB9/vnnOn78uLp166aePXuqRYsWKlu2rPbt2+c4r3jx4o7XJUqUuGxcu92uypUrKyoqSpKUlZWlU6dOyWazybIsx3GXnj3Li4oVK6pUqVJyd3d3jOnn56ewsDBJ0rlz55SVlaVSpUopOjpa3377rTZv3qzAwEBFR0erVKlS19YcAABwRfwUZD7w8/NTeHi4atWqJReXi5m2ZMmSuueee9SnTx/Vrl1bX375pbKysvI8ZpUqVZSUlKQffvhBkrR8+XINGzZMHh4eqlixojZt2iRJ+uyzz6677kaNGmn9+vU6ffq0LMvS2LFjtWDBAm3YsEEhISFq2bKlwsLCVLJkSR0/fvy65wEAANlxBywftGrVSqNGjdKQIUMc21xdXWW329W+fXtZlqVHHnnE8VB7XhQrVkyzZs3ShAkTdOHCBbm5uWny5MmSpKlTp2rkyJGaOXOm6tWrd911V6tWTS+++KJ69+4tu92u6tWrq3///nJyctK6devUoUMHFS9eXJ06dZK3t/d1zwMAALKzWX9fzwLyQd/x6xSXmGpkrtXhAf+65xt4pqPg0eOCR4/NoM8Fi2fAoA8//FArVqy4bLuXl5fee++9QqgIAADkhAB2i+jTp4/69OlT2GUAAIA84CF8AAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIbxUUTId/PDHjc2V9qFTGNzAQCQXwhgyHenT6fIbrcKuwwAAG5aLEECAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMMylsAvArcfT062wS1DahUwln00t7DIAALgiAhjyXd/x6xSXWLjhZ3V4gJILtQIAAHLGEiQAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGJZrAIuNjVWtWrUUEBCgzp07q0OHDnrmmWd04sQJE/VJkkaOHKljx45d9/m//vqrvL29tXbt2nysKmdnz57V0KFD5e/vL39/f/Xt21eHDx/Ol7EDAgKu6fgRI0YoMjIyX+YGAAD5I093wLy8vBQVFaWVK1cqOjpa3t7emjJlSkHX5hATEyPLsq77/OXLl6tdu3b65JNP8rGqnIWHh6tq1apavXq1Vq9ercDAQP3nP//Jl7GjoqLyZRwAAFB4XK7npEaNGmn69Ony9fVVnTp1tG/fPn388cfatGmTPvjgA9lsNtWsWVOjR4/W7bffrsaNG+uxxx7Tzp07dfvtt2vatGmqXLmydu/erYkTJyotLU0eHh4aN26c7rrrLgUHB6t06dI6ePCgunTpori4OPXv319DhgzRBx98oCVLlkiSIiMj9dNPP2ncuHE51pqRkaHVq1dr8eLFeuqpp/Tnn3/q7rvvlqTL6t+yZYsWLFggu92umjVr6rXXXlPx4sX10UcfKSoqSqmpqXJ1dVV4eLiqVKmS45ynTp2Sp6en7Ha7nJyc1L59e5UsWVKSlJWVpSlTpmj79u3KyspSUFCQ+vTpo5iYGM2dO1eurq6KjY2Vr6+vSpYsqS+//FKSNG/ePJUtW1be3t46cOCA5syZI0l66aWXHNeycOFCbd++XStWrNCZM2fUqlUrSdKmTZv00UcfKSMjQwMHDlT79u2VkpKiV199VSdPnlRcXJwaN26sCRMmaPv27Xr33XdVokQJ/f777/L29ta0adNUrFix6/lWAQAAV3DNASwjI0Nr165VvXr19O2336pFixaaOXOmDhw4oLlz52rp0qWOMBUREaHQ0FAlJCSofv36ev3117Vo0SKNHz9es2fPVlhYmObOnauKFStqy5YtGj16tD788ENJkre3tyIiIiRJS5Ys0bx581SpUiVNmTLFEaJWrlypoUOHXrXezZs3q2LFirrvvvvUpk0bffLJJwoJCXHsv1T/wYMHtXTpUi1ZskTFixdXeHi45s+fr169eunLL7/UokWLVKJECc2aNUuLFy/W6NGjc5xz4MCBGjRokD7++GP5+PioadOm6tSpkyRp6dKlkqQVK1YoPT1dffv2Va1atSRJP/30k6Kjo1WmTBk1adJEoaGhioyM1MiRIxUdHa3evXvn6Wt08uRJff7553JxcdGIESOUmpqqpUuX6vTp0+rSpYseeeQRxcTEqHr16po9e7bS09PVoUMH/fLLL5KknTt36osvvpCXl5eefPJJffPNN/L19c3T3DeTcuXcC7uEAnMrX9vNgh4XPHpsBn2+OeUpgMXFxTmePUpPT1edOnU0dOhQffvtt6pbt64k6fvvv1erVq3k4eEhSerWrZtGjhwpSSpevLg6d+4sSQoMDNT06dN1+PBhHT16VAMHDnTMk5KS4nhdp06dy+qw2WwKDAzUqlWrFBQUpNOnTzvmz8ny5cvVsWNHSVL79u01bNgwDRkyxHFH59L5MTExOnLkiJ588klJF4NmjRo15ObmpvDwcEVHR+vw4cPasmWLqlevftU5a9WqpQ0bNmjHjh367rvv9N///ldLlizRJ598oq1bt2rfvn3atm2bJOn8+fM6cOCAHnjgAVWtWlUVKlSQJHl4eKhx48aSpIoVK+rs2bNXnfPvatSoIReX/31pAwMD5eLiovLly6tevXr66aef1LFjR+3evVsffvih/vjjD505c0bnz5+XJD344IO68847JUn333+/kpKS8jz3zSQ+PrmwSygQ5cq537LXdrOgxwWPHptBnwuWk5NNnp5u13VungLYpWfArqR48eKSJLvdnm27ZVnKzMz8/wU6yWazOY5zdnaW3W5X5cqVHeNmZWXp1KlTjvNLlChxxfkCAwP13HPPqVixYrk+kH769Glt2bJFv/zyixYuXCjLsnT27FmtX79eHTp0yFZ/VlaW/Pz8FBYWJkk6d+6csrKydPz4cQUHB6tnz55q0aKFypYtq3379uU4p2VZGjt2rF599VU1bNhQDRs21KBBg9S2bVvt3btXWVlZCgkJ0eOPPy5JSkhI0O23365du3bJ1dU121jOzs45zmOz2bL1PCMjw/H6n737+zh2u12urq5atGiR1q5dqyeffFJNmjTRr7/+6njO7lJPLs1zI8/fAQCAy+Xbr6Fo2LChNm7cqDNnzki6uNTWqFEjSVJqaqo2btwo6eJzWy1atFCVKlWUlJSkH374QdLFO1XDhg274tjOzs7KysqSJFWqVEl33nmnlixZkmsAi4qKko+Pj77++mtt3LhRX331lZ5//nnHM2R/16hRI61fv16nT592hKgFCxbo559/1j333KM+ffqodu3a+vLLLx21XInNZtPvv/+u+fPnOwJSbGysMjMzdffdd8vHx0dLly5VRkaGzp07p+7du2vXrl1XvY4r8fDw0G+//SZJ2r17t+Lj43M8Njo6WpZl6dixY9qzZ49q166tb7/9Vt26dVOnTp104cIF7d+//7IQDQAACsZ1PYR/JdWqVdOAAQMUHBysjIwM1axZM9vD8WvWrNGMGTPk5eWlyZMnq1ixYpo1a5YmTJigCxcuyM3NTZMnT77i2C1btlT//v31/vvv66677lL79u21bt06lS9f/qo1rVix4rKfPuzRo4fef/99/f7775fV/+KLL6p3796y2+2qXr26+vfvr8zMTP3f//2f2rdvL8uy9Mgjj+jgwYNXnXf69OmaOHGiWrdurdtuu03u7u4KDw9XmTJl9NRTT+nIkSMKDAxUZmamgoKC1KhRI8XExFx1zH9q37691q5dq/bt26tmzZqqUaNGjseWLFlSQUFByszM1Ouvv6477rhDvXv31tixYzVv3jy5ubmpfv36io2NdfyAAgAAKDg2y8D60qWf3MsPmZmZGj58uNq1a+dYxsPNpe/4dYpLTC3UGlaHB9yyzz3wTEfBo8cFjx6bQZ8LVoE/A3azsCxLzZs3V5MmTdSmTRtJ0g8//KA33njjisfPmzcv17tk12vo0KGOJcC/8/X11ZAhQwpkTgAAcGswEsDy6+6XzWbT1q1bs21r0KBBofxy0vDwcONzAgCAWwOfBQkAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAsCL1WZAoGuaHFf6HpKddyCzsEgAAyBEBDPnu9OkU2e1WYZcBAMBNiyVIAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIbZLMuyCrsIAABgTtqFTCWfTS3sMoo8JyebPD3drutcl3yuBVDf8esUl8h/2ABws1odHqDkwi7iX44lSAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwrMgHsJSUFI0bN04dO3ZUQECAgoOD9csvv1z3eMHBwYqJidHPP/+sUaNGSZKWLl2qzz777KrnRUZGasSIEdm2xcTEKDg4WJI0a9YsbdiwIcfz8zJHfsjIyFDv3r0VExNzxf1//fWXevTooXbt2mngwIE6d+5cgdcEAMC/TZEOYHa7Xf369VPp0qW1cuVKRUVFadCgQerXr58SExNvaOzatWtrwoQJkqQdO3YoPT39hsYbMmSIWrduneP+/JgjN3/88YeCg4O1c+fOHI8ZN26cunfvrjVr1qhWrVp6++23C7QmAAD+jYp0AIuJidHx48c1ePBgubi4SJJ8fHw0ceJEbd26VU888YSCgoIUGhqqc+fOKTQ0VEFBQQoICHDcbUpPT1dISIj8/Pz03HPPOYLbpbtX3333nTZu3KjZs2dry5Yt113riBEjFBkZqZSUFPXv319BQUEKCgrShg0bLpvj1KlTGjBggPz9/RUYGKivv/5akjRnzhz17dtX7du316JFi9SyZUvZ7XZHvc8999xVa1i2bJmee+451a1b94r7MzIy9P3336tt27aSpKCgIK1Zs+a6rxkAAFyZS2EXcCP27t2ratWqyckpe4589NFHFRMTo8OHD+urr76Su7u7pk2bppo1a2ry5MlKSUnRU089pbp162rdunWSpC+++EKHDx9Wp06dso3VpEkT+fr6qmHDhmrevPlV69m4caMCAgIc78+fP68777wz2zHr169XpUqVNG/ePO3bt0+rVq1SaGhotjmGDBkiHx8fPfPMMzp69KiefvpprVy5UtLFwPj5559LktauXauYmBg1btxYK1euVFBQ0FXrGz58uCRpwYIFV9yfmJgoNzc3R5gtV66cTp48edUxAQDAtSvSAczJyUnFixfPcf99990nd3d3SdJ3332ntLQ0LV++XNLFcHTw4EFt375d3bp1kyTde++9ql+//nXX4+vrq0mTJjnex8TEKCIiItsx9evX1/Tp03Xy5Em1bNlSgwYNumycbdu2afz48ZKku+66S3Xr1tVPP/0kSapTp47juC5dumjVqlWqV6+etm3bprFjx1537ZJkWZZsNlu2bf98DwC4NZQr517YJfyrFekAVqtWLX388ceXBYfp06erSZMmKlGihGOb3W7X1KlTVbNmTUnSqVOnVLp0aS1dulSWZTmOu3T3p6Dce++9+uKLL7RlyxZ99dVX+u9//+u4o3XJ3+u59D4rK0uSsl1Tu3btNGPGDK1du1YtWrS4ahjNizvuuEPJycnKysqSs7Oz4uPj5eXldUNjAgBuTvHxyYVdQpHn5GSTp6fb9Z2bz7UY1aBBA3l6eioiIsIRULZs2aLIyEglJCRkO9bHx0f/93//J0mKi4tTp06ddPz4cTVu3FirV6+W3W7XsWPHtGPHjsvmcXZ2dox/oz766CPNmTNHfn5+eu2115SQkKCUlJRsc/j4+GjZsmWSpKNHj2rHjh2qV6/eZWPddtttatGihaZPn57r8mNeuLq6qkGDBo5AuHLlSrVo0eKGxwUAANkV6TtgNptNb7/9tiZOnKiOHTvKxcVFHh4emjdvnpKTsyf7F198UWPHjlXHjh2VlZWlkJAQ3X333erevbsOHjwoPz8/VapUSVWrVr1sniZNmmj69Olyd3dXu3btbqjmzp0765VXXpG/v7+cnZ0VEhKiUqVKZZtj1KhRGjNmjCIjIyVJ48ePz/FOVIcOHbRjx44cH6zPi1GjRsnX11etW7fWa6+9phEjRuidd95RhQoVNH369OseFwAAXJnN+ud6F4qMrKwszZgxQ56ennrmmWcKuxyHvuPXKS4xtbDLAADkYHV4AEuQ+eBGliCL9B0w0z788EOtWLHisu1eXl567733jNfTpUsXeXh46J133pEk/fnnn3rppZeueOz48eNVu3Ztk+UBAIAccAcM+Y47YABwc+MOWP741z6EDwAAUBQRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYxmdBAgDwL5N2IVPJZ/nM3ht1I58F6ZLPtQA6fTpFdju5vqCUK+fOh+gWMHpc8OixGfT55sUSJAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOAAQAAGEYAAwAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDbJZlWYVdBAAAQEFLu5Cp5LOp+Taek5NNnp5u13WuS75VAfx/fcevU1xi/n2DAwCQH1aHByi5sIv4/1iCBAAAMIwABgAAYBgBDAAAwDACGAAAgGEEMAAAAMMIYAAAAIYRwAAAAAwjgAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDimQAS0lJ0bhx49SxY0cFBAQoODhYv/zyi5G5g4ODFRMTo59//lmjRo3K17FHjhypY8eOSZJGjBihyMjIfB0fAADcHIpcALPb7erXr59Kly6tlStXKioqSoMGDVK/fv2UmJhorI7atWtrwoQJ+TpmTEyMLMvK1zEBAMDNx6WwC7hWMTExOn78uAYPHiwnp4v50cfHRxMnTpTdbtfcuXO1atUqOTs7q2nTpgoJCZGzs7NmzJihrVu3KikpSV5eXpoxY4bKli2rxo0b67HHHtPOnTt1++23a9q0aapcubJ8fX3Vrl07fffdd5KkN998UzVq1MhWR0REhBYtWqR9+/ZpzJgxSktLU+nSpTVt2jSVLVtWY8eO1cGDB3Xq1Cl5e3tr+vTpOnXqlF588UU9+OCD2rdvnzw9PTVr1iwtXbpUcXFx6t+/vxYvXpztmn19fdWpUyd98803Sk1N1eTJk1WrVq0rznvnnXdesQfHjx/XoEGDVKVKFf3222+qUaOG6tevrxUrVigpKUlvvfWW7r//fu3evVsTJ05UWlqaPDw8NG7cON11113mvsAAAPwLFLk7YHv37lW1atUc4euSRx99VHv27NHGjRu1fPlyrVixQkeOHNGSJUt05MgR/fHHH1qyZInWrl2rChUqaNWqVZKkhIQE1a9fX6tXr1aHDh00fvx4x5glS5bUypUrNXjwYIWGhuZY07Bhw/TCCy9o9erVat++vRYsWKCdO3fK1dVVn3zyidavX6/k5GRt3rxZkrR//34988wz+uyzz1SqVCmtXr1a/fv3l5eXl+bNmycPD4/L5ihTpoyWLVump556Su+++26O827evPmKPZCkAwcOqF+/foqKitKOHTt07NgxffLJJ+rYsaM++eQTpaenKywsTOHh4VqxYoWeeeYZjR49+sa+YAAA3ETKlXPPt388Pd2uu44idwfMyclJxYsXv+K+bdu2qUOHDrrtttskSV26dNHKlSvVo0cPhYaG6tNPP9WhQ4e0a9cu3X333ZKk4sWLq3PnzpKkwMBATZ8+3THek08+KeniHagRI0YoISHhsjkTEhIUHx+vVq1aSZK6d+/u2FemTBktXrxYf/zxhw4fPqzz589Lkjw9PR130x588EElJSXlet3Nmzd3HL9u3boc5508efIVe/Doo4+qbNmyjnnvvPNONW7cWJJUsWJFxcbG6vDhwzp69KgGDhzomDclJSXX2gAAKCri45PzbSwnJ9t1h7AiF8Bq1aqljz/+WJZlyWazObZPnz5dW7duVWBgYLbjMzMztWfPHg0dOlR9+vRR27Zt5eTk5HjWysnJyTGO3W6Xs7Oz41wXl/+155/7LnF1dc1Wx4ULFxQXF6dff/1Vs2fPVq9evRQUFKTExETHnH8PkDabLU/PfV0659JcOc1rt9svOzczM1OSVKxYsWzb/3k9drtdlStXVlRUlCQpKytLp06dyrU2AABwbYrcEmSDBg3k6empiIgIZWVlSZK2bNmiyMhI9e7dW9HR0UpLS1NmZqaWL18uHx8fff/992rYsKGefvpp3Xvvvdq0aZPj3NTUVG3cuFGSFBkZqRYtWjjmio6OliStX79e999/v0qXLn1ZPe7u7ipfvry++eYbSVJUVJRmzZqlrVu3ys/PT126dFGpUqUUExPjmDMnzs7OuR6T27w+Pj5X7EFeVKlSRUlJSfrhhx8kScuXL9ewYcPydC4AAMi7IncHzGaz6e2339bEiRPVsWNHubi4yMPDQ/PmzVONGjV0/PhxdenSRZmZmWrWrJl69uyp06dP68UXX5S/v7+ki3fRYmNjHWOuWbNGM2bMkJeXlyZPnuzYvmPHDi1btky33XabJk2alGNNU6dO1dixYzV16lR5eHhoypQpSkxM1LBhwxQdHS1XV1c99NBD2ea8kpYtW6p///56//3389SLK83r5eWlffv2XdaDEydO5DpesWLFNGvWLE2YMEEXLlyQm5tbtn4AAID8YbP+5b/3wNvbWwcOHLhsu6+vrxYuXKjKlSsXQlVFW9/x6xSXmFrYZQAAkM3q8ICb5hmwIrcECQAAUNQVuSXI/Halu1+SHM+FAQAA5DfugAEAABhGAAMAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAw7F//YdwAAODfIe1CppLPpubbeDfyYdz/+s+CRP47fTpFdju5vqCUK+eu+Pjkwi7jlkaPCx49NoM+37xYggQAADCMAAYAAGAYAQwAAMAwAhgAAIBhBDAAAADDCGAAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADHMp7AJw63FyshV2Cbc8elzw6HHBo8dm0OeCcyO9tVmWZeVjLQAAAMgFS5AAAACGEcAAAAAMI4ABAAAYRgADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhhHAAAAADCOA4bqsXr1a7du31+OPP67Fixdftn/fvn0KCgpS27ZtNWrUKGVmZhZClUVbbj2+ZPjw4YqMjDRY2a0jtx5/+eWXCggIUKdOnfTCCy8oKSmpEKos2nLr8fr16+Xv768OHTpoxIgRSk9PL4Qqi768/nmxadMm+fr6Gqzs1pFbjyMiItSqVSsFBAQoICDgql8HSZIFXKMTJ05YrVq1shITE61z585Z/v7+1sGDB7Md06FDB2vnzp2WZVnWyJEjrcWLFxdCpUVXXnp84sQJa8CAAVadOnWs5cuXF1KlRVduPU5OTraaNm1qnThxwrIsy5o5c6b1xhtvFFa5RVJuPT537pzVrFkzKz4+3rIsy3r55ZetJUuWFFa5RVZe/rywLMuKj4+32rVrZ7Vq1aoQqiza8tLjAQMGWDt27MjzmNwBwzX77rvv5OPjozJlyqhkyZJq27at1qxZ49h/7NgxpaWlqV69epKkoKCgbPuRu9x6LF3821jr1q3l5+dXSFUWbbn1OCMjQ6+99prKly8vSfL29tbx48cLq9wiKbcelyxZUhs3blTZsmWVmpqq06dPq1SpUoVYcdGUlz8vJCksLEwvvvhiIVRY9OWlx3v27NG7774rf39/vf7667pw4cJVxySA4ZrFxcWpXLlyjvdeXl46efJkjvvLlSuXbT9yl1uPJem5555T165dTZd2y8itxx4eHnrsscckSWlpaZo3b57atGljvM6iLC/fx66urtq8ebNatmypxMRENWvWzHSZRV5e+rxw4ULVqFFDdevWNV3eLSG3Hp87d07Vq1dXSEiIVqxYobNnz+rtt9++6pgEMFwzu90um83meG9ZVrb3ue1H7uhhwctrj5OTk9W/f39Vq1ZNgYGBJkss8vLa40cffVQxMTFq1aqVxo4da7DCW0Nuff7111+1bt06vfDCC4VR3i0htx7ffvvteu+993T//ffLxcVFzz77rDZv3nzVMQlguGZ33nmn4uPjHe/j4+Pl5eWV4/5Tp05l24/c5dZj3Li89DguLk7du3eXt7e3JkyYYLrEIi+3Hp85c0bffPON472/v78OHDhgtMZbQW59XrNmjeLj49WlSxf179/f8X2NvMutx3/99ZeWLVvmeG9ZllxcXK46JgEM16xJkybaunWrEhISlJqaqnXr1qlFixaO/ZUqVVLx4sX1448/SpKioqKy7UfucusxblxuPc7KytLzzz8vPz8/jRo1ijuQ1yG3HluWpZCQEP3111+SLgaFhx56qLDKLbJy6/PgwYO1du1aRUVFad68efLy8tLHH39ciBUXPbn1uESJEpo6daqOHj0qy7K0ePFixyMMObl6PAOuoHz58vrPf/6jXr16KSMjQ0888YTq1Kmjfv36afDgwapdu7amTZumsLAwpaSkqGbNmurVq1dhl12k5KXHuDG59fjEiRPau3evsrKytHbtWklSrVq1uBN2DfLyffzGG29owIABstlseuCBBzRu3LjCLrvI4c+LgpeXHr/++usaOHCgMjIy9NBDD+mZZ5656pg2y7IsQ/UDAABALEECAAAYRwADAAAwjAAGAABgGAEMAADAMAIYAACAYQQwAAAAwwhgAAAAhv0/TN0/sV4JnqQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# random forest for feature importance on a classification problem\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "t = train.copy()\n",
    "y = t.pop(target)\n",
    "X = t\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "\n",
    "###\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(35).plot(kind='barh')\n",
    "plt.title(\"Top 15 important features\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # summarize feature importance\n",
    "# for i,v in enumerate(importance):\n",
    "# \tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# # plot feature importance\n",
    "# pyplot.bar([x for x in range(len(importance))], importance)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = [\"Credit_History_1.0\",\"Property_Area_Semiurban\",\"Married_Yes\",\"CoapplicantIncome\"]\n",
    "new_train = [\"Credit_History_1.0\",\"Property_Area_Semiurban\",\"Married_Yes\",\"CoapplicantIncome\", \"Loan_Status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sending data for baseline test (entering data_split.py):\n",
    "\n",
    "train[new_train].to_csv(\"new_train_final.csv\", index = False)\n",
    "test[new_test].to_csv(\"new_test_final.csv\", index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary(df = train, target = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"loan_comp.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv(\"submission_test56.csv\")\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##UNIVARIATE VISUAL ANALYSIS:\n",
    "#Check tha distribution of each feature and draw useful information\n",
    "#We will use the separation we made (continuous, discrete, categorical) to plot accodingly \n",
    "\n",
    "#Points to check:\n",
    "\n",
    "#Cómo es la forma de la distribucion? #es una distribucion simetrica?\n",
    "#\"The shape of the distribution is a fundamental characteristic of your sample\n",
    "#that can determine which measure of central tendency best reflects the center of your data\".\n",
    "#Also the shape also impacts your choice between using a parametric or nonparametric hypothesis test.\n",
    "#Cuál es el valor más típico\n",
    "#Lo datos estàn dispersos o no?\n",
    "#Si no, cómo es?\n",
    "#El promedio estará alterado por la presencia de outliers , como acá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1. CONTINUOUS FEATURES\n",
    "#For continuous data we must use the histogram\n",
    "\n",
    "#LoanAmount\n",
    "g = sns.displot(train.LoanAmount, bins = 25,  binwidth=20)\n",
    "g.fig.set_figwidth(12)\n",
    "g.fig.set_figheight(5.7)\n",
    "plt.title(\"Mode: {}, median :{}, mean:{}\".format(train.LoanAmount.mode()[0], train.LoanAmount.median(), np.round(train.LoanAmount.mean(),2)))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cómo es la forma de la distribucion? #es una distribucion simetrica? No es simétrica, right-skewed\n",
    "#\"The shape of the distribution is a fundamental characteristic of your sample\n",
    "#that can determine which measure of central tendency best reflects the center of your data\".\n",
    "#Also the shape also impacts your choice between using a parametric or nonparametric hypothesis test.\n",
    "#Cuál es el valor más típico: mode: 120\n",
    "#Lo datos estàn dispersos o no?: Existen outliers evidentes en el extremo derecho, que tiran el promedio hacia arriba\n",
    "#Si no, cómo es?\n",
    "#El promedio estará alterado por la presencia de outliers , como acá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ApplicantIncome\n",
    "g = sns.displot(train, x= \"ApplicantIncome\", bins = 30)\n",
    "g.fig.set_figwidth(12)\n",
    "g.fig.set_figheight(5.7)\n",
    "plt.title(\"Mode: {}, median :{}, mean:{}\".format(train.ApplicantIncome.mode()[0], train.ApplicantIncome.median(), train.ApplicantIncome.mean()))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cómo es la forma de la distribucion? #es una distribucion simetrica?: Es simétrica, con un peak a la izquierda a analizar\n",
    "#y además right skewed con algunos valores altos (alrededor de 80)\n",
    "#\"The shape of the distribution is a fundamental characteristic of your sample\n",
    "#that can determine which measure of central tendency best reflects the center of your data\".\n",
    "#Also the shape also impacts your choice between using a parametric or nonparametric hypothesis test.\n",
    "#Cuál es el valor más típico: Mode = 24 años\n",
    "#Lo datos estàn dispersos o no?: Hay dispersión hacia la derecha, pero en gral se ve simétrica\n",
    "#Si no, cómo es?\n",
    "#El promedio estará alterado por la presencia de los outliers , como acá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age and Sex: male\n",
    "t = train[train.Sex == \"male\"]\n",
    "g = sns.displot(train[train.Sex == \"male\"], x= \"Age\", bins = 30)\n",
    "g.fig.set_figwidth(12)\n",
    "g.fig.set_figheight(5.7)\n",
    "plt.title(\"Mode: {}, median :{}, mean:{}\".format(t.Age.mode()[0], t.Age.median(), t.Age.mean()))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age and Sex: female\n",
    "t = train[train.Sex == \"female\"]\n",
    "g = sns.displot(t, x= \"Age\", bins = 30)\n",
    "g.fig.set_figwidth(12)\n",
    "g.fig.set_figheight(5.7)\n",
    "plt.title(\"Mode: {}, median :{}, mean:{}\".format(t.Age.mode()[0], t.Age.median(), t.Age.mean()))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ANALYSIS:\n",
    "# Male: Symetric distribution with right skewness. More disperse ages, with typical value of 19\n",
    "#Female: Symetric distribution with right skewness.Les disperse than men, with typical value of 24 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisemos además los valores de la izquierda, correspondientes a los menores:\n",
    "children = train[train.Age <14]\n",
    "g = sns.displot(children, x= \"Age\", bins = 25)\n",
    "g.fig.set_figwidth(12)\n",
    "g.fig.set_figheight(5.7)\n",
    "plt.title(\"Mode: {}, median :{}, mean:{}\".format(children.Age.mode()[0], children.Age.median(), children.Age.mean()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Niños: distribución\n",
    "#Valor típico:mode: 2 años\n",
    "#Distribución trimodal, con peaks en 2,4, y 9 años como los más frecuentes valores\n",
    "#Valores dispersos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. DISCRETE FEATURES\n",
    "#num_discrete = ['PassengerId', 'Survived', 'Pclass', 'SibSp','Parch']\n",
    "\n",
    "#Here I can use the countplot (barplot) to check the distribution\n",
    "\n",
    "#SURVIVED\n",
    "g = sns.countplot(x= \"Survived\" ,data= train)\n",
    "plt.title(\"{}: {}% survived\".format(\"Survived\", np.round(train[train.Survived == 1].shape[0]/ train.shape[0], 2)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pclass\n",
    "g = sns.countplot(x= \"Pclass\" ,data= train)\n",
    "train.Pclass.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mas de la mitad de la gente estaba en la 3ra clase\n",
    "\n",
    "\n",
    "#SibSp\n",
    "g = sns.countplot(x= \"SibSp\" ,data= train)\n",
    "train.SibSp.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La mayoria de la gente (91%) viajaba sin esposas/hermanos o a lo más declaraba 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parch\n",
    "#Parch\n",
    "g = sns.countplot(x= \"Parch\" ,data= train)\n",
    "train.Parch.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mayoritariamente vemos viajando gente con a lo más 1 padre/niño (76% declara 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importante: vemos que podría establecerse una categoría GRUPO FAMILIAR = Persona + Sibsp + Parch para cada pasajero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = train.copy()\n",
    "t2[\"Family\"] = t2[\"Parch\"] * t2[\"SibSp\"] \n",
    "data_summary(df = t2.select_dtypes(include = \"number\"), target = \"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En principio la correlación obtenida con target no es mejor que la de Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisemos el resto de las variables, que son categoricas\n",
    "\n",
    "categ_nominal = ['Sex', \"Name\",'Ticket', 'Cabin', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex\n",
    "g = sns.countplot(x= \"Sex\" ,data= train)\n",
    "train.Sex.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2/3 de los pasajeros eran de sexo masculino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embarked\n",
    "g = sns.countplot(x= \"Embarked\" ,data= train)\n",
    "train.Embarked.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Más del 72% de la gente se embarcó desde S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cabin, ticket y name los dejaré para revisarlos más adelante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RESUMEN OBSERVACIONES UNIVARIATE ANALYSIS\n",
    "\n",
    "#We see outliers in Age and Fare, that are worthwhile to check and see if they can be treated\n",
    "##Mayoritariamente vemos viajando gente con a lo más 1 padre/niño (76% declara 0) (Parch)\n",
    "##Importante: vemos que podría establecerse una categoría GRUPO FAMILIAR = Persona + Sibsp + Parch para cada pasajero\n",
    "#Mas de la mitad de la gente estaba en la 3ra clase\n",
    "#Distribución trimodal DE NIÑOS, con peaks en 2,4, y 9 años como los más frecuentes valores\n",
    "# 2/3 de los pasajeros eran de sexo masculino\n",
    "# Más del 72% de la gente se embarcó desde S\n",
    "#Edad típica de hombres menor que mujeres y con mayor dispersión (0-80 versus 0-65 aprox)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIVARIATE ANALYSIS: Check the relationships of the features with the target variable\n",
    "\n",
    "#Summary:\n",
    "num_discrete = ['PassengerId', 'Survived', 'Pclass', 'SibSp',\n",
    "       'Parch']\n",
    "num_continuous = ['Age',  'Fare']\n",
    "categ_nominal = ['Sex', \"Name\",'Ticket', 'Cabin', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEX/SURVIVED\n",
    "\n",
    "#General view\n",
    "g = sns.countplot(x= \"Sex\" ,data= train, hue = \"Survived\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[(train.Sex == \"male\")].Survived.value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[(train.Sex == \"female\")].Survived.value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let´s look into the distribution of sex/survival/age\n",
    "survived = 'survived'\n",
    "not_survived = 'not survived'\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1,figsize=(10, 15))\n",
    "women = train[train['Sex']=='female']\n",
    "men = train[train['Sex']=='male']\n",
    "ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
    "ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
    "ax.legend()\n",
    "ax.set_title('Female')\n",
    "ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
    "ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=50, label = not_survived, ax = axes[1], kde = False)\n",
    "ax.legend()\n",
    "_ = ax.set_title('Male')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(women[women['Survived']==1].Age.mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mujeres: mayor prob surv. entre 15 y 35 años. Mayor dispersión de edades de supervivientes que de fallecidos\n",
    "#Mujeres: fallecidas curva asimetrica con multiples valores tipicos, pero que no superan fecuencias de 5 en cada caso\n",
    "#Hombres: Fallecidos muestra curva right- skew con dispersión importante de edades. Mayor cantidad de fallecidos entre\n",
    "#los 17 y 35 años aproximadamente\n",
    "#Hombres sobrevivientes: Entre los 18 y 40 años mayor probabilidad de supervivencia, Entre 60 y 75 no hay chance (pero si en mujeres). \n",
    "#Hombres: valor típico de supervivencia niños 0-5 años\n",
    "\n",
    "#Puede ser interesante generar bins de edades para ver su correlación, ya que se ven claramente rangos másmenos expuestos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##distribution of sex/survival/Embarked\n",
    "survived = 'survived'\n",
    "not_survived = 'not survived'\n",
    "# fig, axes = plt.subplots(nrows=4, ncols=1,figsize=(10, 15))\n",
    "women = train[train['Sex']=='female']\n",
    "men = train[train['Sex']=='male']\n",
    "\n",
    "g = sns.countplot(x= \"Embarked\",data= women[women['Survived']== 0])\n",
    "g.set_title(\"Women survived by Embarked\")\n",
    "plt.show()\n",
    "g = sns.countplot(x= \"Embarked\",data= women[women['Survived']== 1])\n",
    "g.set_title(\"Women not-survived by Embarked\")\n",
    "\n",
    "plt.show()\n",
    "g = sns.countplot(x= \"Embarked\",data= men[men['Survived']== 0])\n",
    "g.set_title(\"Men survived by Embarked\")\n",
    "\n",
    "plt.show()\n",
    "g = sns.countplot(x= \"Embarked\",data= men[men['Survived']== 1])\n",
    "g.set_title(\"Men not-survived by Embarked\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Análisis porcentual de cada caso:\n",
    "#Se calcula que S = 31% mujeres fallecidas, Q = 25% y C = 12,3%\n",
    "women[women.Survived == 1].groupby([\"Embarked\"]).Survived.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Análisis porcentual de cada caso:\n",
    "#Se calcula que S = 82,5% hombres fallecidos, Q = 92% y C = 69%\n",
    "men[men.Survived == 1].groupby([\"Embarked\"]).Survived.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notable es que el menor % de fallecidos tanto en hombres como mujeres se ve en los que se embarcaron en C. Por qué?\n",
    "\n",
    "#Podemos crear un feature = EMBARKED+ _+ SEX (correlación con target de -0.37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.PCLASS\n",
    "#Veamos la distribución de edades de sobrevivientes/no sobrevivientes según Pclass \n",
    "\n",
    "grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=5.5, aspect=1.6)\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "grid.add_legend();\n",
    "\n",
    "\n",
    "##Se aprecia la mayor dispersión y cantidad de personas fallecidas en la 3ra clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cHECKING PCLASS, SEX, SURVIVED\n",
    "\n",
    "survived = 'survived'\n",
    "not_survived = 'not survived'\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1,figsize=(10, 15))\n",
    "women = train[train['Sex']=='female']\n",
    "men = train[train['Sex']=='male']\n",
    "ax = sns.distplot(women[women['Survived']==1].Pclass.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
    "ax = sns.distplot(women[women['Survived']==0].Pclass.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
    "ax.legend()\n",
    "ax.set_title('Female')\n",
    "ax = sns.distplot(men[men['Survived']==1].Pclass.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
    "ax = sns.distplot(men[men['Survived']==0].Pclass.dropna(), bins=50, label = not_survived, ax = axes[1], kde = False)\n",
    "ax.legend()\n",
    "_ = ax.set_title('Male')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Claramente las mayores pprobabilidades de supervivencia de mujeres son en 1ra y 2da clase. En 3ra es de 50%\n",
    "#La mayor probabilidad de fallecimiento de hombres está en 3ra clase \n",
    "\n",
    "#INTERESANTE CREAR UN NUEVO FEATURE PCLASS +\"_\"+SEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE FOR ENGINEERING A NEW CATEGORICAL FEATURE \n",
    "# t = train.copy()\n",
    "# t[\"port_sex\"] = t.Pclass.astype(\"str\") +\"_\"+ t.Sex.astype(\"str\")\n",
    "# t.port_sex = t.groupby([\"port_sex\"])[\"PassengerId\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex/Sibsp/Survived\n",
    "\n",
    "survived = 'survived'\n",
    "not_survived = 'not survived'\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1,figsize=(10, 15))\n",
    "women = train[train['Sex']=='female']\n",
    "men = train[train['Sex']=='male']\n",
    "ax = sns.distplot(women[women['Survived']==1].SibSp.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
    "ax = sns.distplot(women[women['Survived']==0].SibSp.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
    "ax.legend()\n",
    "ax.set_title('Female')\n",
    "ax = sns.distplot(men[men['Survived']==1].SibSp.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
    "ax = sns.distplot(men[men['Survived']==0].SibSp.dropna(), bins=50, label = not_survived, ax = axes[1], kde = False)\n",
    "ax.legend()\n",
    "_ = ax.set_title('Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos que las mujeres con 0 ó 1 SibSp tienen mayores probabilidades de sobrevivir. A partir de 5 ya no lo logran\n",
    "#Igualmente los hombres, aunque con muchos menos sobrevivientes. Hay correlación aparente ahí\n",
    "#INTERESANTE CREAR UN NUEVO FEATURE CON SIBSP+SEX: tiene un corr de -0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train.copy()\n",
    "t[\"port_sex\"] = t.SibSp.astype(\"str\") +\"_\"+ t.Sex.astype(\"str\")\n",
    "t.port_sex = t.groupby([\"port_sex\"])[\"PassengerId\"].transform(\"count\")\n",
    "t.port_sex.corr(t.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agreguemos una variante estudiando un feature de familiy = Parch + SibSp\n",
    "t = train.copy()\n",
    "t[\"Family\"] = t[\"SibSp\"] + t[\"Parch\"]\n",
    "\n",
    "survived = 'survived'\n",
    "not_survived = 'not survived'\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1,figsize=(10, 15))\n",
    "women = t[t['Sex']=='female']\n",
    "men = t[t['Sex']=='male']\n",
    "ax = sns.distplot(women[women['Survived']==1].Family.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
    "ax = sns.distplot(women[women['Survived']==0].Family.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
    "ax.legend()\n",
    "ax.set_title('Female')\n",
    "ax = sns.distplot(men[men['Survived']==1].Family.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
    "ax = sns.distplot(men[men['Survived']==0].Family.dropna(), bins=50, label = not_survived, ax = axes[1], kde = False)\n",
    "ax.legend()\n",
    "_ = ax.set_title('Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se repite la tendencia\n",
    "#INTERESANTE CREAR UN FEATURE FAMILY = SIBSP + PARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping passengerId, ya que no aporta \n",
    "# train.PassengerId.corr(train.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop(\"PassengerId\", axis =1, inplace = True)\n",
    "# test.drop(\"PassengerId\", axis =1, inplace = True)\n",
    "# df.drop(\"PassengerId\", axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##WORK WITH NULLS AND MISSING DATA\n",
    "nulls_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NULLS IN CABIN\n",
    "df.Cabin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##En el barco había 8 decks, cada uno con una letra . La cabina comenzaba por el deck correspondiente\n",
    "##Por eso es posible hacer un nuevo feature que asigne el deck correspondiente a cada persona\n",
    "#Se armará un feature llamado Deck, en base a la letra de cada deck, que luego se convierte en int\n",
    "#Los nan quedarán como 0\n",
    "\n",
    "import re\n",
    "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "df['Cabin'] = df['Cabin'].fillna(\"U\") #antes se rellenan los nan con U para que no molesten\n",
    "df['Deck'] = df['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group()) #extrae la letra del Cabin\n",
    "\n",
    "#Creo un feature que cuenta las ocurrencias de cada cabin por id\n",
    "df[\"countDeck\"] = df.groupby([\"Deck\"])[\"PassengerId\"].transform(\"count\")\n",
    "\n",
    "#Además tendremos el feat de mapeo numérico de Deck\n",
    "# df['Deck'] = df['Deck'].map(deck)\n",
    "# df['Deck'] = df['Deck'].fillna(0)\n",
    "# df['Deck'] = df['Deck'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df[\"Survived\"] !=-1].Deck.corr(df[df[\"Survived\"] !=-1].Survived) #igual sirve porque tiene buena correlación con target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se elimina Cabin y se actualizan df, train y test\n",
    "df.drop([\"Cabin\"], axis =1, inplace = True)\n",
    "train = df[df[\"Survived\"] !=-1]\n",
    "test = df[df[\"Survived\"] ==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NULLS IN AGE\n",
    "#Hay 263 nulls en total para el feature Age\n",
    "\n",
    "#TRUCO CHINO: Probaré un truco consistente en calcular el mean de Age del train set, el std delAge del test set y reemplazar\n",
    "#todos los valores nan de Age con números random generados a +- 1 std del mean\n",
    "#Nota:uso el mean como valor típico dado que Age tiene una distribución bastante simétrica (ver statistics with Jim)\n",
    "\n",
    "data = [train, test]\n",
    "\n",
    "for dataset in data:\n",
    "    mean = train.Age.mean()\n",
    "    std = test.Age.std()\n",
    "    nr_nulls = dataset.Age.isnull().sum() #nro de nulls de Age en cada set\n",
    "    #genero los nros random de enteros según la regla\n",
    "    rand_age = np.random.randint(mean-std, mean + std, size = nr_nulls)\n",
    "    #Fill de Nan\n",
    "    age_slice = dataset.Age.copy()\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    dataset.Age = age_slice\n",
    "    dataset.Age = dataset.Age.astype(int)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vuelvo a unir train y test en df\n",
    "df = pd.concat([train, test], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMBARKED\n",
    "#Como embarked tiene solo 2 valores nan, hago fill con su valor tipico\n",
    "df.Embarked.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Embarked = df.Embarked.fillna(\"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fare\n",
    "#Segun Jim, dado que Fare tiene una distribución con skewed y es un dato continuo, reemplazo con la mediana\n",
    "df.Fare = df.Fare.fillna(df.Fare.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora que imputé todos los nulls, actualizo train y test\n",
    "train = df[df[\"Survived\"] !=-1]\n",
    "test = df[df[\"Survived\"] ==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTLIERS!!!!!\n",
    "#Treebased models are robust to outliers, so we won´t them in this project\n",
    "#Source: https://www.quora.com/Why-are-tree-based-models-robust-to-outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CONVERT AND ENCODE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CUANDO USAR LabelEncoding:\n",
    "#De acuerdo al libro de Abishek, se puede usar en todos los Tree based models (dt, rf, extra trees) o bien\n",
    "# en todos los boosted trees models (xgboost, GBM, lightGBM)\n",
    "#NO USAR EN LINEAR MODELS, SVM O EN NN, DADO QUE ELLOS NECESITAN CONSUMIR LOS DATOS NORMALIZADOS O ESTANDARIZADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como acá vamos a trabajar con tree-based y boosted trees, así que podemos usar el LabelEncoding sin problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CONVERTIR DATA TYPES Y ENCODING\n",
    "#Convert Fare from float to int\n",
    "df.Fare = df.Fare.astype(\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Sex\n",
    "# dict = {\"male\": 0, \"female\":1}\n",
    "\n",
    "# df.Sex =  df.Sex.map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Embarked\n",
    "#Usaremos label encoding\n",
    "# dict = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
    "\n",
    "# df.Embarked =  df.Embarked.map(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ticket\n",
    "df.Ticket.value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay demasiados valores unicos como para que valga la pena convertirlos,d e manera que los eliminamos\n",
    "ticket_backup = df.Ticket #respaldo\n",
    "df.drop([\"Ticket\"], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name\n",
    "#Se crea un nuevo feature llamado title, extrayendo los títulos de cada pasajero y transformándolos encategorías\n",
    "#Luego se mapea cada categoría con un int para transformarla en numérica con LabelEncoding\n",
    "\n",
    "\n",
    "df[\"Title\"] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)# extraen los str que terminen en un punto\n",
    "#Reemplazo los menor frecuentes con un \"Rare\"\n",
    "df.Title = df.Title.replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
    "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "#Ajusto y reemplazo mme, ms, me con Miss y Mrs\n",
    "df.Title = df.Title.replace([\"Ms\", \"Mlle\"], \"Miss\")\n",
    "df.Title = df.Title.replace(\"Mme\", \"Mrs\")\n",
    "\n",
    "df.to_csv(\"titanic.csv\")\n",
    "\n",
    "# #Mapeamos para hacer un LabelEncoding\n",
    "# dict = {'Mr':1, 'Miss':2,'Mrs':3,  'Master':4, 'Rare':5}\n",
    "# df.Title = df.Title.map(dict)\n",
    "# #Por si acaso se rellena cualquier nan con 0\n",
    "# df.Title = df.Title.fillna(0)\n",
    "\n",
    "#Ahora elimino Name\n",
    "df.drop(\"Name\", axis = 1, inplace = True)\n",
    "#Actualizo train y test\n",
    "train = df[df[\"Survived\"] !=-1]\n",
    "test = df[df[\"Survived\"] ==-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hagamos un primer test del output del EDA\n",
    "# train.to_csv(\"../input/train_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"Survived\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FEATURE ENGINEERING\n",
    "#LAB EXPERIMENT: WE WILL CREATE AS MUCH FEATURES AS WE CAN AND LET DE EATURE IMPORTANCE DECIDE which ones are the best\n",
    "##NEW FEATURES:\n",
    "# GRUPO FAMILIAR = Persona + Sibsp + Parch para cada pasajero\n",
    "# #Podemos crear un feature = EMBARKED+ _+ SEX (correlación con target de -0.37)\n",
    "# #INTERESANTE CREAR UN NUEVO FEATURE PCLASS +\"_\"+SEX\n",
    "# INTERESANTE CREAR UN NUEVO FEATURE CON SIBSP+SEX: tiene un corr de -0.35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Family\n",
    "df.Family = 1 + df.SibSp + df.Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SibSp+Sex (usamos la técnica de Abishek: unimos primero como caregoricas, luego sacamos sus frcuencias y a cada\n",
    "#una se le asigna la frecuencia)\n",
    "df[\"port_sex\"] = df.SibSp.astype(\"str\") +\"_\"+ df.Sex.astype(\"str\")\n",
    "df.port_sex = df.groupby([\"port_sex\"])[\"Deck\"].transform(\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pclass+Sex (usamos la técnica de Abishek: unimos primero como caregoricas, luego sacamos sus frcuencias y a cada\n",
    "#una se le asigna la frecuencia)\n",
    "df[\"port_sex\"] = df.Pclass.astype(\"str\") +\"_\"+ df.Sex.astype(\"str\")\n",
    "df.port_sex = df.groupby([\"port_sex\"])[\"Deck\"].transform(\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embarked+Sex (usamos la técnica de Abishek: unimos primero como caregoricas, luego sacamos sus frcuencias y a cada\n",
    "#una se le asigna la frecuencia)\n",
    "df[\"port_sex\"] = df.Embarked.astype(\"str\") +\"_\"+ df.Sex.astype(\"str\")\n",
    "df.port_sex = df.groupby([\"port_sex\"])[\"Deck\"].transform(\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otros features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age*Pclass\n",
    "df.AgeClass = df.Age * df.Pclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actualizo los datasets\n",
    "train = df[df[\"Survived\"] !=-1]\n",
    "test = df[df[\"Survived\"] ==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train\n",
    "%store test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let´s check...\n",
    "#Hasta el momento (14/11): mejor accuracy 0.78947 usando GradientBoosting y los features. Lugar 1.600\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nexto to work on: Ensemble learning and new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.65 + 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FEATURES (22/11/2020)\n",
    "#url: https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FARE binning\n",
    "#Convertir a bins los rangos de fare\n",
    "new_feats['rank'] = new_feats['Fare'].rank(method='first')\n",
    "\n",
    "new_feats['Fare'] = pd.qcut(new_feats[\"rank\"].values, 13).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats.drop(\"rank\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##AGE BINNING\n",
    "new_feats['rank'] = new_feats['Age'].rank(method='first')\n",
    "new_feats['Age_bin'] = pd.qcut(new_feats[\"rank\"].values, 10).codes\n",
    "new_feats.drop(\"rank\", axis = 1, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Family size\n",
    "new_feats[\"FamilySize\"] = 1 + new_feats.SibSp + new_feats.Parch\n",
    "\n",
    "#Family Size with 1 are labeled as Alone\n",
    "#Family Size with 2, 3 and 4 are labeled as Small\n",
    "#Family Size with 5 and 6 are labeled as Medium\n",
    "#Family Size with 7, 8 and 11 are labeled as Large\n",
    "\n",
    "\n",
    "def assign(s):\n",
    "    if s ==1:\n",
    "        s = \"Alone\"\n",
    "    elif 2<= s <=4:\n",
    "        s = \"Small\"\n",
    "    elif 5<= s <= 6:\n",
    "        s= \"Medium\"\n",
    "    elif 7<= s <= 11:\n",
    "        s =\"Large\"\n",
    "    return s\n",
    "        \n",
    "\n",
    "\n",
    "new_feats.FamilySize = new_feats.FamilySize.apply(assign)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le aplico el método de Abihsek para que se convierta en frecuencias\n",
    "\n",
    "new_feats[\"Count_FamilySize\"] = new_feats.groupby([\"FamilySize\"])[\"Deck\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Ticket frquency\n",
    "new_feats[\"Ticket\"] = ticket_backup\n",
    "new_feats[\"Ticket_freq\"] = new_feats.groupby(\"Ticket\")[\"Ticket\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats.Ticket_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feats = new_feats[['Age_bin', 'FamilySize', 'Ticket_freq']] ##version final new feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenar y crear nuevo datset \n",
    "df2 = pd.concat((df,new_feats), axis = 1)\n",
    "df2.drop(\"Age\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicated columns\n",
    "df2 = df2.loc[:,~df2.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Nuevos train y test\n",
    "train2 = df2[df2.Survived!= -1]\n",
    "test2 = df2[df2.Survived == -1]\n",
    "# test2.drop(\"Survived\", axis = 1, inplace = True)\n",
    "# df2.drop(\"Survived\", axis = 1, inplace = True) #Dejo el df2 sin el target para procesar todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df2[[\"PassengerId\", \"Parch\", \"Sex\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = a.pop(\"Sex\")\n",
    "y.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save the data for using in pipeline Framework\n",
    "df2.to_csv(\"titanic_total.csv\", index = False)\n",
    "train2.to_csv(\"titanic_train.csv\", index = False)\n",
    "test2.to_csv(\"titanic_test.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####FEAT IMPORTANCE ORIGINAL\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import config\n",
    "import os\n",
    "import joblib \n",
    "import glob\n",
    "\n",
    "import yellowbrick\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "\n",
    "##FEATURE IMPORTANCE\n",
    "\n",
    "def feat_importance(all_files, X_train, y_train):\n",
    "    # nrows = 3\n",
    "    # ncols = 1\n",
    "    # fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n",
    "        names_classifiers = []\n",
    "        nclassifier = 0\n",
    "        # all_files = glob.glob(\"../models\" + \"/*.bin\")\n",
    "        for filename in all_files:\n",
    "                model = joblib.load(filename)\n",
    "                names_classifiers.append((filename, model.best_estimator_))\n",
    "\n",
    "        for name, model in names_classifiers:\n",
    "            viz = FeatureImportances(model)\n",
    "            viz.fit(X_train, y_train)\n",
    "            viz.show()\n",
    "\n",
    "\n",
    "    # for row in range(nrows):\n",
    "    #     print(row)\n",
    "    #     for col in range(ncols):\n",
    "    #         print(\"Col:\", col)\n",
    "    #         name = names_classifiers[nclassifier][0]\n",
    "    #         classifier = names_classifiers[nclassifier][1]\n",
    "    #         indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n",
    "    #         g = sns.barplot(y=X_train.columns[indices][:40], \n",
    "    #         x = classifier.feature_importances_[indices][:40] , \n",
    "    #         orient='h',ax=axes[row][col])\n",
    "    #         g.set_xlabel(\"Relative importance\",fontsize=12)\n",
    "    #         g.set_ylabel(\"Features\",fontsize=12)\n",
    "    #         g.tick_params(labelsize=9)\n",
    "    #         g.set_title(name + \" feature importance\")\n",
    "    #         nclassifier += 1\n",
    "    #         print(\"ready col\", col)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df = pd.read_csv(config.DF)\n",
    "    target = config.TARGET\n",
    "    num_folds = config.FOLDS\n",
    "    kfold = StratifiedKFold(n_splits = num_folds)\n",
    "    seed = config.SEED\n",
    "    X_train =  pd.read_csv(\"../input/X_train.csv\")\n",
    "    y_train =  pd.read_csv(\"../input/y_train.csv\").values.ravel()\n",
    "    scoring = config.SCORING\n",
    "    all_files = glob.glob(\"../models/bestModels\" + \"/*.bin\")\n",
    "    feat_importance(all_files, X_train, y_train)\n",
    "\n",
    "    \n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn import svm, datasets\n",
    " from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    " iris = datasets.load_iris()\n",
    " parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    " svc = svm.SVC()\n",
    " clf = GridSearchCV(svc, parameters)\n",
    " clf.fit(iris.data, iris.target)\n",
    "gs = GridSearchCV(estimator=svc,\n",
    "             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
    " sorted(clf.cv_results_.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.std_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_[\"std_test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
