{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae21ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "#display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "#display(HTML(\"<style>.prompt { display:none !important; }</style>\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25056eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "\n",
    "    def __init__(self, sizes, activations, Loss, epochs, metric, learning_rate):\n",
    "        self.weights =[np.random.randn(sizes[i],sizes[i-1]) for i in range(1, len(sizes))]\n",
    "        self.biases =  [np.zeros((1, sizes[i])) for i in range(1, len(sizes))]\n",
    "        self.activations = activations\n",
    "        self.Loss = Loss\n",
    "        self.epochs = epochs\n",
    "        self.metric = metric\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        self.activated_layers = [x]\n",
    "        for w,b, act in zip(self.weights, self.biases, self.activations):\n",
    "            activation = act.activate(np.dot(x, w.T) + b)\n",
    "            self.activated_layers.append(activation)\n",
    "            x = activation\n",
    "            \n",
    "            \n",
    "    def backpropagate(self, y):\n",
    "        #Input: activated_layers\n",
    "        #output: Container Gradient dE/dw \n",
    "        #Initialize\n",
    "        sigmas_box = []\n",
    "        sigma_prime_box = []\n",
    "    \n",
    "       #Backprop the error ()\n",
    "        #1. Compute output layer sigma\n",
    "        loss_grad = self.Loss.loss_gradient(self.activated_layers[-1], y)\n",
    "        output_sigma = self.activations[-1].output_layer_sigma(loss_grad, self.activated_layers[-1])\n",
    "        sigmas_box = [output_sigma]\n",
    "        #Sigmas of the rest of layers...\n",
    "        for w,a,o_layer in zip(self.weights[::-1], self.activations[:-1][::-1],self.activated_layers[:-1][::-1]) :\n",
    "            sigmas_box.append(np.dot(sigmas_box[-1], w) * a.sigma_prime(o_layer))\n",
    "       \n",
    "        #Reverse sigma_box    \n",
    "        sigmas_box.reverse()\n",
    "        #Biases update\n",
    "        self.grad_biases = sigmas_box\n",
    "        #Gradient (dE/dw):\n",
    "        self.gradients = []\n",
    "        for a, s in zip(self.activated_layers[:-1], sigmas_box):\n",
    "            self.gradients.append(np.dot(s.T, a))\n",
    "        #Nota: al hacerse la multiplicacion de todos los inputs a la vez\n",
    "        #Igual se mantiene el shape de cada weight pero mientras más\n",
    "        #Inputs más grandes salen los valores de cada componente de la matriz\n",
    "        #Por eso después se divide cada weight por el total de inputs (mean)\n",
    "        # print(\"Gradients shapes:\")\n",
    "        # for g in gradients:\n",
    "        #     print(g.shape)\n",
    "       \n",
    "\n",
    "    def weight_update(self):\n",
    "        for w,gw, b, gb in zip(self.weights, self.gradients, self.biases, self.grad_biases):\n",
    "            w -= (self.learning_rate / len(X))* gw\n",
    "            b -= (self.learning_rate / len(gb))* np.sum(gb, axis= 0)\n",
    "\n",
    "        return self.weights, self.biases\n",
    "        \n",
    "    \n",
    "    def train(self, X,y):\n",
    "        print(\"Training......\")\n",
    "        for e in range(self.epochs):\n",
    "            #Dar un forward pass y testear\n",
    "            self.forward(X)\n",
    "            #print(\"Testing output layers:\", self.activated_layers[-1][:5])\n",
    "            #Compute the error\n",
    "            error = np.mean(self.Loss.forward_loss(self.activated_layers[-1], y))\n",
    "            if e % 1 == 0:\n",
    "                print(\"Error epoch {0}/{1} : {2}---Accuracy: {3}\".format(e,self.epochs,\n",
    "                                                                         error, self.metric.get_accuracy(self.activated_layers[-1], y)))\n",
    "            #Backprop the error ()\n",
    "            self.backpropagate(y)\n",
    "            self.weights, self.biases = self.weight_update()\n",
    "        print(\"Training done!\")\n",
    "\n",
    "\n",
    "            \n",
    "    def SGD(self, X,y,x_test, y_test, minibatch_size):\n",
    "        \"\"\"Vectorized version\"\"\"\n",
    "        print(\"SGD Training......\")\n",
    "        for e in range(1, self.epochs + 1):\n",
    "            #tomar dataset y generar minibatches box\n",
    "            minibatches = self.minibatch_generator(X,y, minibatch_size)\n",
    "            Losses = []\n",
    "            Accuracies = []\n",
    "            for mb in minibatches:\n",
    "                nabla_w, nabla_b = [], [] #box para ir guardando los dC/dw y dC/db de cada ejemplo\n",
    "                input = mb[0]\n",
    "                y_true = np.array(mb[1]).astype(int)\n",
    "                #Dar un forward pass \n",
    "                #print(\"Minibatch input shape:\", input.shape)\n",
    "                #print(\"Minibatch y_true shape:\", y_true.shape)\n",
    "                #print(\"Bias\", self.biases[0])\n",
    "\n",
    "                \n",
    "                self.forward(input)\n",
    "                #Calcular el error\n",
    "                error = np.mean(self.Loss.forward_loss(self.activated_layers[-1], y_true))\n",
    "                #Guardar el error y accuracy\n",
    "                Losses.append(error)\n",
    "                Accuracies.append(self.metric.get_accuracy(self.activated_layers[-1], y_true))\n",
    "\n",
    "\n",
    "                #Obtener los dC/dw y dC/db del minibatch usando backprop\n",
    "                self.backpropagate(y_true)\n",
    "                delta_nw = self.gradients #dC/dw\n",
    "                delta_nb = self.grad_biases #dC/db\n",
    "                self.weights = [w - (self.learning_rate/ len(mb)) * dw for w,dw in zip(self.weights, delta_nw)]\n",
    "                self.biases = [b - (self.learning_rate/ len(mb)) * np.sum(db, axis = 0) for b, db in zip(self.biases, delta_nb)]\n",
    "            \n",
    "            #Reporte de error epoch...\n",
    "            if (e % 100 == 0 ) or (e == self.epochs):\n",
    "                print(\"Average Error epoch {0}: {1}---Average Accuracy: {2}\".format(e, np.mean(Losses), np.mean(Accuracies)))\n",
    "                print(\"Accuracy in test set:\", self.evaluate_test(x_test, y_test))\n",
    "                   \n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    \n",
    "            \n",
    "    def Momentum(self, X,y,x_test, y_test, minibatch_size, beta = 0.9):\n",
    "        \"\"\"Vectorized version\"\"\"\n",
    "        print(\"SGD Training......\")\n",
    "        for e in range(1, self.epochs + 1):\n",
    "            #tomar dataset y generar minibatches box\n",
    "            minibatches = self.minibatch_generator(X,y, minibatch_size)\n",
    "            Losses = []\n",
    "            Accuracies = []\n",
    "            #Velocities initialization\n",
    "            Vdw = [np.zeros(w.shape) for w in self.weights]\n",
    "            Vdb = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "            for mb in minibatches:\n",
    "                nabla_w, nabla_b = [], [] #box para ir guardando los dC/dw y dC/db de cada ejemplo\n",
    "                input = mb[0]\n",
    "                y_true = np.array(mb[1]).astype(int)\n",
    "                #Dar un forward pass \n",
    "                #print(\"Minibatch input shape:\", input.shape)\n",
    "                #print(\"Minibatch y_true shape:\", y_true.shape)\n",
    "                #print(\"Bias\", self.biases[0])\n",
    "                \n",
    "                self.forward(input)\n",
    "                #Calcular el error\n",
    "                error = np.mean(self.Loss.forward_loss(self.activated_layers[-1], y_true))\n",
    "                #Guardar el error y accuracy\n",
    "                Losses.append(error)\n",
    "                Accuracies.append(self.metric.get_accuracy(self.activated_layers[-1], y_true))\n",
    "\n",
    "                #Obtener los dC/dw y dC/db del minibatch usando backprop\n",
    "                self.backpropagate(y_true)\n",
    "                delta_nw = self.gradients #dC/dw\n",
    "                delta_nb = self.grad_biases #dC/db\n",
    "                \n",
    "                #Compute the exp moving averages (velocities)\n",
    "                Vdw = [beta * vw + (1 - beta) * dnw for vw, dnw in zip(Vdw, delta_nw)]\n",
    "                Vdb = [beta * vb + (1 - beta) * dnb for vb, dnb in zip(Vdb, delta_nb)]\n",
    "                \n",
    "                #Update weights and biases using the Velocities\n",
    "                \n",
    "                self.weights = [w - (self.learning_rate/ len(mb)) * dw for w,dw in zip(self.weights, Vdw)]\n",
    "                self.biases = [b - (self.learning_rate/ len(mb)) * np.sum(db, axis = 0) for b, db in zip(self.biases, Vdb)]\n",
    "            \n",
    "            #Reporte de error epoch...\n",
    "            if (e % 100 == 0 ) or (e == self.epochs):\n",
    "                print(\"Average Error epoch {0}: {1}---Average Accuracy: {2}\".format(e, np.mean(Losses), np.mean(Accuracies)))\n",
    "                print(\"Accuracy in test set:\", self.evaluate_test(x_test, y_test))\n",
    "                   \n",
    "        print(\"Training complete!\") \n",
    "\n",
    "    def minibatch_generator(self, X,y, batch_size):\n",
    "        dataset = list(zip(X,np.array(y)))\n",
    "        np.random.shuffle(dataset)\n",
    "        minibatches = [(X[i:i+batch_size,:], y[i:i+batch_size]) for\n",
    "                        i in range(0, len(y), batch_size)]\n",
    "\n",
    "        #si minibatch final es mas chico que el batch size se le mete desde\n",
    "        #atras inputs hasta completar el tamaño batch size\n",
    "        if len(minibatches[-1][0]) < batch_size:\n",
    "            #print(\"Len minibatches -1:\", len(minibatches[-1][0]))\n",
    "            minibatches[-1] = (X[-batch_size:,:], y[-batch_size:])\n",
    "            \n",
    "        return minibatches\n",
    "    \n",
    "    def evaluate_test(self, x_test, y_test):\n",
    "        \"\"\"Evaluates the model on the test set\n",
    "        input: x_test, y_test\n",
    "        output: accuracy\"\"\"\n",
    "        #Forward pass---obtain prediction y_pred\n",
    "        self.forward(x_test)\n",
    "        #Evaluate prediction with accuracy\n",
    "        acc_test = self.metric.get_accuracy(self.activated_layers[-1], y_test)\n",
    "        #Return accuracy\n",
    "        return acc_test\n",
    "\n",
    "        \n",
    " \n",
    "class Relu():\n",
    "    def activate(self, x):\n",
    "        self.output = np.maximum(0,x)\n",
    "        return self.output\n",
    "    \n",
    "    def sigma_prime(self, x):\n",
    "        return 1. * (x > 0)\n",
    "\n",
    "\n",
    "class Sigmoid():\n",
    "    def activate(self, x):\n",
    "        #np.exp - (x - np.max(x, axis = 1, keepdims= True))\n",
    "        x = np.clip(x, 1e-7, 1 - 1e-7)\n",
    "        self.output = 1 / (1+ np.exp (- (x - np.max(x, axis = 1, keepdims= True))))\n",
    "        #self.output = 1 / (1+ np.exp(-x))\n",
    "        return self.output\n",
    "    \n",
    "    def output_layer_sigma(self, loss_gradients, x):\n",
    "        \"\"\"en realidad calcula todo el sigma de una vez como dC/da * sigma_prime\n",
    "        dC/da = loss_gradient\"\"\"\n",
    "        \n",
    "        self.output_sigma = loss_gradients * self.sigma_prime(x)\n",
    "        return self.output_sigma\n",
    "    \n",
    "    def sigma_prime(self, x):\n",
    "        return x * (1-x)\n",
    "\n",
    "class Softmax():\n",
    "    def activate(self, x):\n",
    "        #Get unnormalized probs\n",
    "        exp_values = np.exp(x - np.max(x, axis = 1, keepdims= True))\n",
    "        #Get normalized probs\n",
    "        self.output = exp_values / np.sum(exp_values, axis= 1, keepdims= True)\n",
    "        return self.output\n",
    "    \n",
    "    def output_layer_sigma(self, loss_gradients, out_activations):\n",
    "        \"\"\"Dado que es complejo multplicar el jacobiano de cada input por\n",
    "        #su loss_gradient por que el jac es una matrix, se hace aca todo directo y se saca \n",
    "        #el output layer sigma = dE/dsigma.dsigma/dz\"\"\"\n",
    "        #Se crea un contenedor donde irá el output_sigma de cada input\n",
    "        #del tamaño del loss_gradient (dinputs)\n",
    "        self.output_sigma = np.empty_like(loss_gradients)\n",
    "\n",
    "        #Tomo uno a uno los Loss_gradientes de cada input y cada\n",
    "        #softmax activation de la output layer para hacer uno a uno los\n",
    "        #output_sigmas...\n",
    "        for index, (single_act, single_loss_grad) in enumerate(zip(out_activations, loss_gradients)):\n",
    "            single_act = single_act.reshape(-1,1)\n",
    "            #Calculate jacobian matrix (sigma_prime of softmax)\n",
    "            jacobian_matrix = np.diagflat(single_act) - np.dot(single_act, single_act.T)\n",
    "            self.output_sigma[index] = np.dot(jacobian_matrix, single_loss_grad)\n",
    "        return self.output_sigma\n",
    "\n",
    "         \n",
    "\n",
    "    \n",
    "##Loss Units\n",
    "class MSE():\n",
    "    \n",
    "    #Forward\n",
    "    def forward_loss(self, y_pred, y_true):\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        labels = len(y_pred[0])                  \n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        return  np.sum((y_pred- y_true)**2, axis=1) / len(y_pred)\n",
    "        \n",
    "    #Derivative\n",
    "    def loss_gradient(self, y_pred, y_true): #dE/dact\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        labels = len(y_pred[0])                  \n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        self.dinputs = (2/len(y_pred)) * (y_pred - y_true)\n",
    "        return self.dinputs\n",
    "    \n",
    "    \n",
    "class CategoricalCrossEntropyLoss():\n",
    "    def forward_loss(self, y_pred, y_true):\n",
    "         #entrega el vector de negative losses de cada sample\n",
    "         y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7) #recorta para evitar logs mulas\n",
    "         if len(y_true.shape) == 1: #si el y_true viene en un solo vector de escalares\n",
    "             #extraigo el valor que tiene el indice indicado en el y_true\n",
    "             #correspondiente\n",
    "             correct_confidences = y_pred[range(len(y_pred)), y_true]\n",
    "        \n",
    "         if len(y_true.shape) == 2: #matrix\n",
    "             #lo mismo pero multiplique y sume para obtener el valor\n",
    "             #que tiene el indice indicado por el y_true (el resto se hace zero\n",
    "             #al multiplicar)\n",
    "             correct_confidences = np.sum( y_pred * y_true, axis = 1)\n",
    "        \n",
    "         negative_loss_likehoods = -np.log(correct_confidences)\n",
    "\n",
    "         return negative_loss_likehoods\n",
    "    \n",
    "    def loss_gradient(self, dvalues, y_true): #dE/dact\n",
    "        # Number of samples\n",
    "        dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "\n",
    "class Accuracy():\n",
    "#gets the accuracy of the training stage\n",
    "    def get_accuracy(self, y_pred, y_true):\n",
    "        #saca el indice donde esta el valor mas grande\n",
    "        predictions = np.argmax(y_pred, axis= 1)\n",
    "\n",
    "        #y_true en formato escalares\n",
    "        if len(y_true.shape) == 1:\n",
    "            accuracy = np.mean(predictions == y_true)\n",
    "        #matrix\n",
    "        elif len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis= 1)\n",
    "            accuracy = np.mean(predictions == y_true) #promedia coincidencias de valor de indice\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "\n",
    "###TESTING MODULE\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "\n",
    "X, y = datasets.make_classification(n_samples = 50000, n_features = 8,n_redundant=0,n_informative= 5,\n",
    "                                      n_classes = 3)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "# if __name__ == '__main__':\n",
    "#     sizes = [8, 10, 3]\n",
    "#     EPOCHS = 50\n",
    "#     #sizes = [4, 5,  3]\n",
    "#     net = Dense(sizes, activations = [Relu(), Softmax()], Loss = CategoricalCrossEntropyLoss(),\n",
    "#                 epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.015)\n",
    "#     # net.train(X_scaled,y)\n",
    "#     net.SGD(X_scaled,y, minibatch_size = 10)\n",
    "\n",
    "\n",
    "\n",
    "##NEXT....................................................\n",
    "#REVIEW AND UNDERSTAND THE BACKPROP AND PARAMETER UPDATING MECHANICS\n",
    "#BIAS UPDATE ESTA MUY CUTRE\n",
    "#REARRANGE, EDIT DEAD CODE AND RESTACK THE CODE\n",
    "#hacer un codigo explicativo de aprendizaje y generar un codigo terminado para produccion\n",
    "#NEXT LEVEL: SGD\n",
    "#test with train, test split\n",
    "#Benchmarking with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c4efe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"mnist_train.csv\", delimiter= \",\")\n",
    "X = train.iloc[:, 1:]\n",
    "y = train.iloc[:, 0]\n",
    "\n",
    "#Loading test set\n",
    "test = pd.read_csv(\"mnist_test.csv\", delimiter= \",\")\n",
    "x_test = test.iloc[:, 1:]\n",
    "y_test = test.iloc[:, 0]\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "# X = scaler.transform(X)\n",
    "# test_X = scaler.transform( test_X )\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform( X )\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b684cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes2 = [784,30, 10]\n",
    "EPOCHS = 3000\n",
    "#sizes2 = [4, 5,  3]\n",
    "net = Dense(sizes2, activations = [Sigmoid(), Softmax()], Loss = CategoricalCrossEntropyLoss(),\n",
    "            epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.9)\n",
    "#net = Dense(sizes2, activations = [Sigmoid(), Softmax()], Loss = MSE(),\n",
    "            #epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.07)\n",
    "#net.train(X,y)\n",
    "\n",
    "#net.SGD(X , np.array(y), x_test, y_test, minibatch_size = 10)\n",
    "#net.SGD(X[:10000] , y[:10000], x_test, y_test, minibatch_size = 10)\n",
    "#net.Momentum(X[:10000] , y[:10000], x_test, y_test, minibatch_size = 10, beta = 0.9)\n",
    "#net.SGD(X[:1000] , y[:1000], x_test, y_test, minibatch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc6e249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = Dense(sizes2, activations = [Sigmoid(), Softmax()], Loss = MSE(),\n",
    "            epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.05)\n",
    "        \n",
    "net2 = Dense(sizes2, activations = [Sigmoid(), Softmax()], Loss = CategoricalCrossEntropyLoss(),\n",
    "            epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "be8a6c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GRADIENT CHECKING MODULE\n",
    "def get_params(net):\n",
    "    \"\"\"Returns an unique array containing the value of all weights together (rolled)\"\"\"\n",
    "    return np.concatenate([w.ravel() for w in net.weights])\n",
    "\n",
    "def set_weights(net, weight_vector):\n",
    "        #Reshapes weight_vector with the original weights shape\n",
    "        passed = 0\n",
    "        resized_vector = []\n",
    "        for i in range(len(net.weights)):\n",
    "            vector = np.array(weight_vector[passed: passed+ net.weights[i].size])\n",
    "            passed += net.weights[i].size\n",
    "            resized_vector.append(vector.reshape(net.weights[i].shape))\n",
    "        return resized_vector\n",
    "        \n",
    "def forward_for_checking(net, inputs, weights,biases):\n",
    "        x = inputs\n",
    "        activated_layers = [x]\n",
    "        for w,b, act in zip(weights, biases, net.activations):\n",
    "            activation = act.activate(np.dot(x, w.T) + b)\n",
    "            activated_layers.append(activation)\n",
    "            x = activation\n",
    "        return activated_layers\n",
    "\n",
    "\n",
    "def numerical_gradient(net, X, y, epsilon):\n",
    "    \"\"\"Returns a vector of unravel gradients of each wij\n",
    "    [dw11,dw12,....,dwij...]\"\"\"\n",
    "    params = get_params(net)#vector of the gradients for each wij\n",
    "    num_grad = np.zeros(params.shape)\n",
    "    perturb = np.zeros(params.shape)\n",
    "    for i in range(len(params)): #for each wij\n",
    "        perturb[i] = epsilon\n",
    "        w_plus = set_weights(net, params + perturb)\n",
    "        y_pred_plus = forward_for_checking(net, X, w_plus, net.biases)[-1]\n",
    "        loss_plus = net.Loss.forward_loss(y_pred_plus,y).mean()\n",
    "        w_minus = set_weights(net, params - perturb)\n",
    "        y_pred_minus = forward_for_checking(net, X, w_minus, net.biases)[-1]\n",
    "        loss_minus = net.Loss.forward_loss(y_pred_minus,y).mean()\n",
    "        #Assigns the gradient of wij\n",
    "        num_grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        perturb[i] = 0\n",
    "    #Reshapes the gradient to original shape\n",
    "    #num_grad = set_weights(num_grad, weights)\n",
    "    #print(\"Length of the numerical gradient vector:\", num_grad.shape)\n",
    "    return num_grad\n",
    "\n",
    "\n",
    "def gradient_checking(net, X, y, epsilon):\n",
    "    \n",
    "    num_gradient = numerical_gradient(net, X, y,epsilon)\n",
    "    #Computing the gradient via backprop...\n",
    "    y_pred = net.forward(X)\n",
    "    net.backpropagate(y)\n",
    "    backprop_gradients = net.gradients\n",
    "    backprop_gradients = np.concatenate([g.ravel() for g in backprop_gradients])\n",
    "\n",
    "    #Calculate and evaluate\n",
    "    numerator = np.linalg.norm(backprop_gradients - num_gradient)\n",
    "    print(\"Numerator:\", numerator)\n",
    "    denominator = np.linalg.norm(backprop_gradients) + np.linalg.norm(num_gradient)\n",
    "    print(\"Denominator:\", denominator)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference < epsilon:\n",
    "        print('The gradient is correct')\n",
    "    else:\n",
    "        print(\"El gradiente es pútrido!!! Puajj!!\")\n",
    "    \n",
    "    print(\"Fórmula alternativa:\", np.linalg.norm(backprop_gradients - num_gradient)/np.linalg.norm(backprop_gradients  + num_gradient))\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "af841483",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"mnist_train.csv\", delimiter= \",\")\n",
    "X = train.iloc[:, 1:]\n",
    "y = train.iloc[:, 0]\n",
    "\n",
    "#Loading test set\n",
    "test = pd.read_csv(\"mnist_test.csv\", delimiter= \",\")\n",
    "x_test = test.iloc[:, 1:]\n",
    "y_test = test.iloc[:, 0]\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "# X = scaler.transform(X)\n",
    "# test_X = scaler.transform( test_X )\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform( X )\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "sizes2 = [784,30, 10]\n",
    "EPOCHS = 50\n",
    "\n",
    "net1 = Dense(sizes2, activations = [Sigmoid(), Softmax()], Loss = MSE(),\n",
    "            epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.05)\n",
    "        \n",
    "net2 = Dense(sizes2, activations = [Sigmoid(), Softmax()], Loss = CategoricalCrossEntropyLoss(),\n",
    "            epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.05)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6fa52a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerator: 22.86115316843748\n",
      "Denominator: 24.375397510812544\n",
      "El gradiente es pútrido!!! Puajj!!\n",
      "Fórmula alternativa: 0.9918624782038078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9378781682758868"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gradient_checking(net2, X[:1], y[:1],epsilon = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dc2bed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data \n",
    "y_iris= iris.target\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_iris)\n",
    "\n",
    "X_scaled = scaler.transform(X_iris)\n",
    "\n",
    "sizes = [4, 10, 3]\n",
    "EPOCHS = 50\n",
    "net3 = Dense(sizes, activations = [Relu(), Softmax()], Loss = CategoricalCrossEntropyLoss(),\n",
    "                 epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b787b5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerator: 2.1876048036252172e-08\n",
      "Denominator: 10.478444839562929\n",
      "The gradient is correct\n",
      "Fórmula alternativa: 2.087718967003185e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.087718967003185e-09"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_checking(net3, X_scaled[:1], y_iris[:1],epsilon = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GRADIENT CHECKING MODULE\n",
    "\"\"\"Utility for checking if the backpropagation self-made unit is working properly\n",
    "The module compares the numerically computed gradient with the gradient calculated with the network backprop\n",
    "checking the criteria: norm(backgrad- num_grad) /(norm(backgrad) + norm(num_grad)) < epsilon\n",
    "Parameters:\n",
    "net: instantiation of the network\n",
    "Ex: net2 = Dense(sizes = [4,10,3], activations = [Sigmoid(), Softmax()], Loss = CategoricalCrossEntropyLoss(),\n",
    "            epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.05)\"\"\"\n",
    "\n",
    "class GradientChecking():\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Returns an unique array containing the value of all weights together (rolled)\"\"\"\n",
    "        return np.concatenate([w.ravel() for w in self.net.weights])\n",
    "\n",
    "    def set_weights(self, weight_vector):\n",
    "            \"\"\"Reshapes weight_vector with the original weights shape\"\"\"\n",
    "            passed = 0\n",
    "            resized_vector = []\n",
    "            for i in range(len(self.net.weights)):\n",
    "                vector = np.array(weight_vector[passed: passed+ self.net.weights[i].size])\n",
    "                passed += self.net.weights[i].size\n",
    "                resized_vector.append(vector.reshape(self.net.weights[i].shape))\n",
    "            return resized_vector\n",
    "            \n",
    "    def forward_for_checking(self, inputs, weights,biases):\n",
    "        \"\"\"Performs a forward pass using the parameters of interest\"\"\"\n",
    "            x = inputs\n",
    "            activated_layers = [x]\n",
    "            for w,b, act in zip(weights, biases, self.net.activations):\n",
    "                activation = act.activate(np.dot(x, w.T) + b)\n",
    "                activated_layers.append(activation)\n",
    "                x = activation\n",
    "            return activated_layers\n",
    "\n",
    "\n",
    "    def numerical_gradient(self, X, y, epsilon):\n",
    "        \"\"\"Returns a vector of unravel gradients of each wij\n",
    "        [dw11,dw12,....,dwij...] obtained via the\n",
    "        formula (J(x+e) - J(x-e)/(2*e) for each and everyone of the weights wij\"\"\"\n",
    "        params = self.get_params()#vector of the gradients for each wij\n",
    "        num_grad = np.zeros(params.shape)\n",
    "        perturb = np.zeros(params.shape)\n",
    "        for i in range(len(params)): #for each wij\n",
    "            perturb[i] = epsilon\n",
    "            w_plus = self.set_weights(params + perturb)\n",
    "            y_pred_plus = self.forward_for_checking(X, w_plus, self.net.biases)[-1]\n",
    "            loss_plus = self.net.Loss.forward_loss(y_pred_plus,y).mean()\n",
    "            w_minus = self.set_weights(params - perturb)\n",
    "            y_pred_minus = self.forward_for_checking(X, w_minus, self.net.biases)[-1]\n",
    "            loss_minus = self.net.Loss.forward_loss(y_pred_minus,y).mean()\n",
    "            #Assigns the gradient of wij\n",
    "            num_grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            perturb[i] = 0\n",
    "        return num_grad\n",
    "\n",
    "\n",
    "    def gradient_check(self, X, y, epsilon):\n",
    "        \"\"\"Executes the gradient checking according to the criteria\"\"\"\n",
    "        num_gradient = self.numerical_gradient(X, y, epsilon)\n",
    "        #Computing the gradient via backprop...\n",
    "        y_pred = self.net.forward(X)\n",
    "        self.net.backpropagate(y)\n",
    "        backprop_gradients = self.net.gradients\n",
    "        backprop_gradients = np.concatenate([g.ravel() for g in backprop_gradients])\n",
    "\n",
    "        #Calculate and evaluate\n",
    "        numerator = np.linalg.norm(backprop_gradients - num_gradient)\n",
    "        denominator = np.linalg.norm(backprop_gradients) + np.linalg.norm(num_gradient)\n",
    "        difference = numerator / denominator\n",
    "\n",
    "        if difference < epsilon:\n",
    "            print('The gradient is correct')\n",
    "        else:\n",
    "            print(\"El gradiente es pútrido!!! Puajj!!\")\n",
    "        \n",
    "        print(\"Fórmula alternativa:\", np.linalg.norm(backprop_gradients - num_gradient)/np.linalg.norm(backprop_gradients  + num_gradient))\n",
    "\n",
    "        return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d4da5875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct\n",
      "Fórmula alternativa: 2.087718967003185e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.087718967003185e-09"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = GradientChecking(net3)\n",
    "tt.gradient_check(X = X_scaled[:1], y = y_iris[:1],epsilon = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cdd487b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"mnist_train.csv\", delimiter= \",\")\n",
    "X = train.iloc[:, 1:]\n",
    "y = train.iloc[:, 0]\n",
    "\n",
    "#Loading test set\n",
    "test = pd.read_csv(\"mnist_test.csv\", delimiter= \",\")\n",
    "x_test = test.iloc[:, 1:]\n",
    "y_test = test.iloc[:, 0]\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "# X = scaler.transform(X)\n",
    "# test_X = scaler.transform( test_X )\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform( X )\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "sizes2 = [784,30, 10]\n",
    "EPOCHS = 50\n",
    "\n",
    "net1 = Dense(sizes2, activations = [Sigmoid(), Softmax()], Loss = MSE(),\n",
    "            epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.05)\n",
    "        \n",
    "net2 = Dense(sizes2, activations = [Sigmoid(), Softmax()], Loss = CategoricalCrossEntropyLoss(),\n",
    "            epochs = EPOCHS, metric = Accuracy(), learning_rate = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b09bc9c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w9/sqb6btqs30jdv8h_fy7t6wp80000gn/T/ipykernel_3395/675823905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientChecking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/w9/sqb6btqs30jdv8h_fy7t6wp80000gn/T/ipykernel_3395/1655038110.py\u001b[0m in \u001b[0;36mgradient_check\u001b[0;34m(self, X, y, epsilon)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mnum_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m#Computing the gradient via backprop...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/w9/sqb6btqs30jdv8h_fy7t6wp80000gn/T/ipykernel_3395/1655038110.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, X, y, epsilon)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mperturb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mw_plus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0my_pred_plus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_for_checking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_plus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mloss_plus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_plus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mw_minus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/w9/sqb6btqs30jdv8h_fy7t6wp80000gn/T/ipykernel_3395/1655038110.py\u001b[0m in \u001b[0;36mforward_for_checking\u001b[0;34m(self, inputs, weights, biases)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mactivated_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mactivated_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc = GradientChecking(net2)\n",
    "gc.gradient_check(X, y,epsilon = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46d6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
